# final_ML_personal
Nguyá»…n Thá»‹ Thu Há»“ng - 52100962

# Dá»° ÃN CUá»I Ká»²
# NHáº¬P MÃ”N Há»ŒC MÃY
BÃ€I 1: TrÃ¬nh bÃ y má»™t bÃ i nghiÃªn cá»©u, Ä‘Ã¡nh giÃ¡ cá»§a em vá» cÃ¡c váº¥n Ä‘á» sau:
1.	TÃ¬m hiá»ƒu, so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p Optimizer trong huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y;
2.	TÃ¬m hiá»ƒu vá» Continual Learning vÃ  Test Production khi xÃ¢y dá»±ng má»™t giáº£i phÃ¡p há»c mÃ¡y Ä‘á»ƒ giáº£i quyáº¿t má»™t bÃ i toÃ¡n nÃ o Ä‘Ã³;

## CHÆ¯Æ NG 1 â€“ Tá»”NG QUAN Vá»€ Há»ŒC MÃY
### 1.1 Há»c mÃ¡y lÃ  gÃ¬?
Machine learning (há»c mÃ¡y hay mÃ¡y há»c) lÃ  má»™t nhÃ¡nh con cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o (AI) vÃ  khoa há»c mÃ¡y tÃ­nh. Machine learning sá»­ dá»¥ng dá»¯ liá»‡u, thuáº­t toÃ¡n Ä‘áº§u vÃ o Ä‘á»ƒ tá»± xá»­ lÃ½ cÃ¡c váº¥n Ä‘á» vÃ  liÃªn tá»¥c tá»‘i Æ°u Ä‘á»ƒ táº¡o ra nhá»¯ng phÆ°Æ¡ng Ã¡n xá»­ lÃ½ má»›i hiá»‡u quáº£ hÆ¡n, phÃ¹ há»£p hÆ¡n, giá»‘ng nhÆ° cÃ¡ch thá»©c tá»± há»c cá»§a nÃ£o bá»™ con ngÆ°á»i.

Má»™t cÃ¡ch tá»•ng quÃ¡t, trong cuá»‘n sÃ¡ch Machine Learning cá»§a tÃ¡c giáº£ Tom Mitchell xuáº¥t báº£n nÄƒm 1997, há»c mÃ¡y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau: â€œA computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over timeâ€ (Má»™t chÆ°Æ¡ng trÃ¬nh mÃ¡y tÃ­nh Ä‘Æ°á»£c cho lÃ  há»c Ä‘á»ƒ thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ T tá»« kinh nghiá»‡m E, náº¿u hiá»‡u suáº¥t thá»±c hiá»‡n cÃ´ng viá»‡c T cá»§a nÃ³ Ä‘Æ°á»£c Ä‘o bá»Ÿi chá»‰ sá»‘ hiá»‡u suáº¥t P vÃ  Ä‘Æ°á»£c cáº£i thiá»‡n bá»Ÿi kinh nghiá»‡m E theo thá»i gian).
KhÃ¡c biá»‡t giá»¯a chÆ°Æ¡ng trÃ¬nh láº­p trÃ¬nh truyá»n thá»‘ng vÃ  há»c mÃ¡y.

<img src="picture/1.1.png">

HÃ¬nh 1.1 Minh há»a chÆ°Æ¡ng trÃ¬nh láº­p trÃ¬nh truyá»n thá»‘ng

<img src="picture/1.1.1.png">

HÃ¬nh 1.1 Minh há»a há»c mÃ¡y

Thá»‘ng kÃª vÃ  dá»± Ä‘oÃ¡n lÃ  hai má»¥c Ä‘Ã­ch chÃ­nh cá»§a viá»‡c Ã¡p dá»¥ng machine learning vÃ¬ tháº¿ há»‡ thá»‘ng nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i kháº£ nÄƒng tá»± nghiÃªn cá»©u, cáº£i tiáº¿n báº£n thÃ¢n dá»±a trÃªn nhá»¯ng nguyÃªn lÃ½ Ä‘Æ°á»£c láº­p trÃ¬nh ban Ä‘áº§u. Trong nhiá»u trÆ°á»ng há»£p machine learning sáº½ tá»± Ä‘á» xuáº¥t ra giáº£i phÃ¡p tá»‘i Æ°u mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh trÆ°á»›c. Do Ä‘Ã³, cÃ³ thá»ƒ nÃ³i Machine Learning giá»‘ng nhÆ° má»™t ngÆ°á»i lao Ä‘á»™ng vá»›i kháº£ nÄƒng tá»± há»c, hoÃ n thiá»‡n vÃ  giÃ u kinh nghiá»‡m hÆ¡n theo thá»i gian.

Trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, khi mÃ  kháº£ nÄƒng tÃ­nh toÃ¡n cá»§a cÃ¡c mÃ¡y tÃ­nh Ä‘Æ°á»£c nÃ¢ng lÃªn má»™t táº§m cao má»›i vÃ  lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ Ä‘Æ°á»£c thu tháº­p bá»Ÿi cÃ¡c hÃ£ng cÃ´ng nghá»‡ lá»›n, Machine Learning Ä‘Ã£ tiáº¿n thÃªm má»™t bÆ°á»›c dÃ i vÃ  má»™t lÄ©nh vá»±c má»›i Ä‘Æ°á»£c ra Ä‘á»i gá»i lÃ  Deep Learning (Há»c SÃ¢u). Deep Learning Ä‘Ã£ giÃºp mÃ¡y tÃ­nh thá»±c thi nhá»¯ng viá»‡c tÆ°á»Ÿng chá»«ng nhÆ° khÃ´ng thá»ƒ vÃ o 10 nÄƒm trÆ°á»›c: phÃ¢n loáº¡i cáº£ ngÃ n váº­t thá»ƒ khÃ¡c nhau trong cÃ¡c bá»©c áº£nh, tá»± táº¡o chÃº thÃ­ch cho áº£nh, báº¯t chÆ°á»›c giá»ng nÃ³i vÃ  chá»¯ viáº¿t cá»§a con ngÆ°á»i, giao tiáº¿p vá»›i con ngÆ°á»i, hay tháº­m chÃ­ cáº£ sÃ¡ng tÃ¡c vÄƒn hay Ã¢m nháº¡c, â€¦

<img src="picture/1.2.png">

HÃ¬nh 1.2 Má»‘i quan há»‡ giá»¯a AI, Machine Learning vÃ  Deep Learning

(Nguá»“n: Whatâ€™s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?)

### 1.2 PhÃ¢n loáº¡i Há»c mÃ¡y:

Dá»±a trÃªn cÃ¡c tiÃªu chÃ­ khÃ¡c nhau, ngÆ°á»i ta cÃ³ thá»ƒ phÃ¢n loáº¡i cÃ¡c thuáº­t toÃ¡n Há»c mÃ¡y theo nhiá»u cÃ¡ch khÃ¡c nhau. 

#### 1.2.1 PhÃ¢n loáº¡i theo váº¥n Ä‘á», nhiá»‡m vá»¥ cáº§n giáº£i quyáº¿t:

Dá»±a vÃ o váº¥n Ä‘á», nhiá»‡m vá»¥ cáº§n giáº£i quyáº¿t cá»§a thuáº­t toÃ¡n, ngÆ°á»i ta phÃ¢n loáº¡i cÃ¡c thuáº­t toÃ¡n Há»c mÃ¡y thÃ nh ba loáº¡i:

1.	Há»“i quy (Regression): Giáº£i quyáº¿t bÃ i toÃ¡n dá»± Ä‘oÃ¡n giÃ¡ trá»‹ má»™t Ä‘áº¡i lÆ°á»£ng nÃ o Ä‘Ã³ dá»±a vÃ o giÃ¡ trá»‹ cá»§a cÃ¡c Ä‘áº¡i lÆ°á»£ng liÃªn quan. VÃ­ dá»¥, dá»±a vÃ o cÃ¡c Ä‘áº·c Ä‘iá»ƒm nhÆ° diá»‡n tÃ­ch, sá»‘ phÃ²ng, khoáº£ng cÃ¡ch tá»›i trung tÃ¢mâ€¦Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ cÄƒn nhÃ .
2.	PhÃ¢n lá»›p (Classification): Giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n nháº­n dáº¡ng xem má»™t Ä‘á»‘i tÆ°á»£ng thuá»™c lá»›p nÃ o trong sá»‘ cÃ¡c lá»›p cho trÆ°á»›c. VÃ­ dá»¥, bÃ i toÃ¡n nháº­n diá»‡n chá»¯ viáº¿t, bÃ i toÃ¡n phÃ¢n loáº¡i emailâ€¦thuá»™c cÃ¡c thuáº­t toÃ¡n phÃ¢n lá»›p.
3.	PhÃ¢n cá»¥m (Clustering): Ã tÆ°á»Ÿng cÆ¡ báº£n giá»‘ng vá»›i cÃ¡c thuáº­t toÃ¡n phÃ¢n lá»›p, sá»± khÃ¡c biá»‡t lÃ  á»Ÿ chá»—, trong cÃ¡c bÃ i toÃ¡n phÃ¢n cá»¥m, cÃ¡c cá»¥m chÆ°a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c vÃ  thuáº­t toÃ¡n pháº£i tá»± khÃ¡m phÃ¡ vÃ  phÃ¢n cá»¥m dá»¯ liá»‡u.

<img src="picture/1.3.png">

HÃ¬nh 1.3 CÃ¡c giáº£i thuáº­t Há»c mÃ¡y

(Nguá»“n: https://tailieuhay.vn/tai-lieu/bai-giang-hoc-may-bai-5-cay-phan-loai-va-hoi-quy-nguyen-thanh-tung-7733/ )

#### 1.2.2 PhÃ¢n loáº¡i theo cÃ¡ch mÃ¡y tÃ­nh há»c:

Dá»±a trÃªn cÃ¡ch mÃ¡y tÃ­nh há»c, ngÆ°á»i ta chia cÃ¡c thuáº­t toÃ¡n Há»c mÃ¡y thÃ nh ba loáº¡i:

1.	Há»c táº­p dÆ°á»›i sá»± giÃ¡m sÃ¡t (Supervised learning): Con ngÆ°á»i sáº½ láº­p trÃ¬nh dá»¯ liá»‡u Ä‘áº§u vÃ o bao gá»“m cáº£ cÃ¡ch thá»©c vÃ  phÆ°Æ¡ng Ã¡n mÃ  con ngÆ°á»i mong muá»‘n. PhÆ°Æ¡ng Ã¡n vÃ  Ä‘Ã¡p Ã¡n sáº½ Ä‘Æ°á»£c gáº¯n nhÃ£n, sáº¯p xáº¿p sáºµn vÃ  Machine Learning chá»‰ cáº§n rÃ  soÃ¡t vÃ  tráº£ ra Ä‘Ãºng káº¿t quáº£ cÃ³ trong bá»™ dá»¯ liá»‡u Ä‘Ã£ cÃ³. Tin nháº¯n rÃ¡c Ä‘áº¿n tá»« 1 sá»‘ nguá»“n sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c tÃ¡ch ra khá»i há»™p thÆ° chÃ­nh lÃ  á»©ng dá»¥ng cá»§a machine learning giÃºp phÃ¢n loáº¡i tin nháº¯n trÃªn email.
2.	Há»c táº­p mÃ  khÃ´ng giÃ¡m sÃ¡t (Unsupervised learning): Machine learning chá»‰ Ä‘Æ°á»£c cung cáº¥p cÃ¡c thuáº­t toÃ¡n, cÃ´ng cá»¥ Ä‘á»ƒ tá»± xá»­ lÃ½ mÃ  khÃ´ng biáº¿t trÆ°á»›c káº¿t quáº£. Dá»… tháº¥y nháº¥t viá»‡c á»©ng dá»¥ng cá»§a phÃ¢n loáº¡i nÃ y Ä‘Ã³ lÃ  cÃ¡ nhÃ¢n hÃ³a tráº£i nghiá»‡m khÃ¡ch hÃ ng.Dá»¯ liá»‡u Ä‘áº§u vÃ o bao gá»“m hÃ nh vi, lá»‹ch sá»­ mua mua hÃ ng vÃ  há»‡ thá»‘ng sáº½ dá»± Ä‘oÃ¡n nhá»¯ng sáº£n pháº©m phÃ¹ há»£p vÃ  Ä‘á» xuáº¥t riÃªng cho tá»«ng khÃ¡ch hÃ ng.
3.	Há»c táº­p Ä‘Æ°á»£c giÃ¡m sÃ¡t bÃ¡n pháº§n (Semi-supervised learning): ÄÃ¢y lÃ  phÃ¢n loáº¡i náº±m á»Ÿ giá»¯a cá»§a 2 phÃ¢n loáº¡i trÃªn khi nÃ y dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  1 há»—n há»£p bao gá»“m cáº£ phÆ°Æ¡ng phÃ¡p láº«n Ä‘Ã¡p Ã¡n. Äiá»ƒm khÃ¡c biá»‡t á»Ÿ Ä‘Ã¢y lÃ  phÆ°Æ¡ng Ã¡n vÃ  Ä‘Ã¡p Ã¡n chÆ°a Ä‘Æ°á»£c nhÃ³m láº¡i thÃ nh tá»«ng bá»™. NhÆ° váº­y machine learning pháº£i tá»± tÃ¬m ra cÃ¡ch giáº£i nÃ o tÆ°Æ¡ng thÃ­ch vá»›i Ä‘Ã¡p Ã¡n nÃ o trong bá»™ dá»¯ liá»‡u sáºµn cÃ³.

### 1.3 CÃ¡c bÆ°á»›c cÆ¡ báº£n thá»±c hiá»‡n má»™t thuáº­t toÃ¡n Há»c mÃ¡y:

NhÃ¬n chung, viá»‡c thá»±c hiá»‡n má»™t thuáº­t toÃ¡n Há»c mÃ¡y thÆ°á»ng tráº£i qua cÃ¡c bÆ°á»›c cÆ¡ báº£n sau:

1.	Thu tháº­p dá»¯ liá»‡u â€“ Gathering data/Data collection
2.	Tiá»n xá»­ lÃ½ dá»¯ liá»‡u â€“ Data preprocessing
    1.	TrÃ­ch xuáº¥t dá»¯ liá»‡u â€“ data extraction
    2.	LÃ m sáº¡ch dá»¯ liá»‡u â€“ data cleaning
    3.	Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u â€“ Data transformation
    4.	Chuáº©n hÃ³a dá»¯ liá»‡u â€“ Data normalization
    5.	TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng â€“ Feature extraction
3.	PhÃ¢n tÃ­ch dá»¯ liá»‡u â€“ Data analysis
4.	XÃ¢y dá»±ng mÃ´ hÃ¬nh mÃ¡y há»c â€“ Model building
5.	Huáº¥n luyá»‡n mÃ´ hÃ¬nh â€“ Model training
6.	ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh â€“ Model evaluation
   
Trong táº¥t cáº£ cÃ¡c bÆ°á»›c thÃ¬ viá»‡c thu tháº­p dá»¯ liá»‡u, tiá»n xá»­ lÃ½ vÃ  xÃ¢y dá»±ng bá»™ dá»¯ liá»‡u lÃ  tá»‘n nhiá»u thá»i gian vÃ  cÃ´ng sá»©c nháº¥t. ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng, cÃ³ áº£nh hÆ°á»Ÿng ráº¥t nhiá»u Ä‘áº¿n hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n Há»c mÃ¡y.

### 1.4 á»¨ng dá»¥ng cá»§a Há»c mÃ¡y:

á»¨ng dá»¥ng tá»•ng quÃ¡t:

â€¢	Xá»­ lÃ½ áº£nh
â€¢	PhÃ¢n tÃ­ch vÄƒn báº£n
â€¢	Khai phÃ¡ dá»¯ liá»‡u

á»¨ng dá»¥ng trong thá»±c táº¿:

â€¢	Giáº£i mÃ£ thá»‹ trÆ°á»ng tÃ i chÃ­nh

â€¢	Thay Ä‘á»•i cá»¥c diá»‡n ngÃ nh nÃ´ng nghiá»‡p

â€¢	NÃ¢ng cao hiá»‡u quáº£ vÃ  cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»‹ch vá»¥ ngÃ nh y táº¿

â€¢	CÆ¡ quan nhÃ  nÆ°á»›c cÃ³ thá»ƒ quáº£n lÃ½ tráº­t tá»± xÃ£ há»™i vÃ  Ä‘áº£m báº£o tÃ¬nh hÃ¬nh phÃ¡t triá»ƒn Ä‘áº¥t nÆ°á»›c

## CHÆ¯Æ NG 2 â€“ CÃC PHÆ¯Æ NG PHÃP OPTIMIZER
### 2.1 Tá»•ng quan vá» Optimizer
#### 2.1.1 Optimizer lÃ  gÃ¬?
Optimizer hay Thuáº­t toÃ¡n tá»‘i Æ°u lÃ  cÆ¡ sá»Ÿ Ä‘á»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh Neural Network vá»›i má»¥c Ä‘Ã­ch â€œhá»câ€ Ä‘Æ°á»£c cÃ¡c feature (hay pattern) cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o, Ä‘á»ƒ tá»« Ä‘Ã³ cÃ³ thá»ƒ tÃ¬m má»™t táº­p cÃ¡c trá»ng sá»‘ (weights â€“ w) vÃ  ngÆ°á»¡ng (bias â€“ b) phÃ¹ há»£p hÆ¡n Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh. 

VÃ  cÃ³ thá»ƒ nÃ³i cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u (Optimizition Algorithm) lÃ  má»™t trong nhá»¯ng â€œháº¡t nhÃ¢nâ€ máº¡nh máº½ cá»§a háº§u háº¿t thuáº­t toÃ¡n Machine Learning. ÄÃ¢y má»™t quy trÃ¬nh Ä‘Æ°á»£c thá»±c hiá»‡n láº·p Ä‘i láº·p láº¡i báº±ng cÃ¡ch so sÃ¡nh cÃ¡c giáº£i phÃ¡p khÃ¡c nhau cho Ä‘áº¿n khi tÃ¬m tháº¥y má»™t giáº£i phÃ¡p tá»‘i Æ°u hoáº·c thá»a Ä‘Ã¡ng.

Äá»‘i vá»›i ká»¹ thuáº­t há»c sÃ¢u nÃ³i riÃªng, thuáº­t toÃ¡n tá»‘i Æ°u lÃ  cÃ¡c ká»¹ thuáº­t giÃºp xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh máº¡ng nÆ¡-ron Ä‘á»ƒ tá»‘i Æ°u hÃ³a Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh máº¡ng.

<img src="picture/2.1.png">

HÃ¬nh 2.1 Minh há»a thuáº­t toÃ¡n tá»‘i Æ°u (Optimizer)

#### 2.1.2 Vai trÃ² cá»§a thuáº­t toÃ¡n tá»‘i Æ°u

Trong thuáº­t toÃ¡n há»c mÃ¡y nÃ³i chung vÃ  ká»¹ thuáº­t há»c sÃ¢u nÃ³i riÃªng, thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a (Optimizer) lÃ  má»™t khÃ¢u quan trá»ng khÃ´ng thá»ƒ thiáº¿u. QuÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a thá»±c hiá»‡n xÃ¡c Ä‘á»‹nh hÃ m máº¥t mÃ¡t (loss function) vÃ  sau Ä‘Ã³ tá»‘i thiá»ƒu hÃ³a hÃ m trÃªn báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m tá»‘i Æ°u. Cá»¥ thá»ƒ, thÃ´ng qua viá»‡c cáº­p nháº­t cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh (w, b) vÃ  Ä‘Ã¡nh giÃ¡ láº¡i hÃ m máº¥t mÃ¡t vá»›i má»™t tá»‰ lá»‡ há»c (learning rate) xÃ¡c Ä‘á»‹nh, quÃ¡ trÃ¬nh tá»‘i Æ°u giÃºp mÃ´ hÃ¬nh tÆ°Æ¡ng thÃ­ch tá»‘t hÆ¡n vá»›i táº­p dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘Ã o táº¡o.

##### 2.1.2.1 HÃ m máº¥t mÃ¡t (Loss function)

HÃ m máº¥t mÃ¡t (Loss function) lÃ  má»™t phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ Ä‘á»™ hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n â€œhá»câ€ cho mÃ´ hÃ¬nh trÃªn táº­p dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng. 
HÃ m máº¥t mÃ¡t tráº£ vá» má»™t sá»‘ thá»±c khÃ´ng Ã¢m thá»ƒ hiá»‡n sá»± chÃªnh lá»‡ch giá»¯a hai Ä‘áº¡i lÆ°á»£ng: 

â€¢	a: nhÃ£n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n

â€¢	y: nhÃ£n Ä‘Ãºng

Báº£n thÃ¢n hÃ m máº¥t mÃ¡t chÃ­nh lÃ  má»™t cÆ¡ cháº¿ thÆ°á»Ÿng-pháº¡t, mÃ´ hÃ¬nh sáº½ pháº£i Ä‘Ã³ng pháº¡t má»—i láº§n dá»± Ä‘oÃ¡n sai vÃ  má»©c pháº¡t tá»‰ lá»‡ thuáº­n vá»›i Ä‘á»™ lá»›n sai sÃ³t. 
Trong má»i bÃ i toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t, má»¥c tiÃªu luÃ´n bao gá»“m giáº£m tá»•ng má»©c pháº¡t pháº£i Ä‘Ã³ng. Trong trÆ°á»ng há»£p lÃ½ tÆ°á»Ÿng a = y, hÃ m máº¥t mÃ¡t sáº½ tráº£ vá» giÃ¡ trá»‹ cá»±c tiá»ƒu báº±ng 0. 
Hai hÃ m máº¥t mÃ¡t thÆ°á»ng xuyÃªn Ä‘Æ°á»£c sá»­ dá»¥ng trong máº¡ng nÆ¡-ron: 

â€¢	MSE (Mean Squared Error)

â€¢	Cross Entropy

##### 2.1.2.2 Tá»‰ lá»‡ há»c (Learning rate)

Learning rate hay tá»‰ lá»‡ há»c lÃ  má»™t thÃ´ng sá»‘ quan trá»ng trong viá»‡c quyáº¿t Ä‘á»‹nh tá»‘c Ä‘á»™ há»c cá»§a máº¡ng nÆ¡-ron. Tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c thá»ƒ hiá»‡n báº±ng sá»± thay Ä‘á»•i giÃ¡ trá»‹ cáº­p nháº­t trá»ng sá»‘ (weights - w) trong cÃ¡c chu ká»³ há»c. TÃ¹y theo má»¥c Ä‘Ã­ch cá»§a mÃ´ hÃ¬nh mÃ  tÄƒng/ giáº£m tá»‰ lá»‡ há»c. 

Tá»‰ lá»‡ há»c cÃ ng cao thÃ¬ giÃºp mÃ´ hÃ¬nh há»c khÃ¡ nhanh vÃ  tiáº¿t kiá»‡m Ä‘Æ°á»£c thá»i gian huáº¥n luyá»‡n, tuy nhiÃªn viá»‡c tá»‰ lá»‡ há»c lá»›n Ä‘á»“ng nghÄ©a vá»›i viá»‡c sá»± thay Ä‘á»•i trá»ng sá»‘ (weights - w) vÃ  tham sá»‘ ngÆ°á»¡ng (bias - b) cÃ ng lá»›n, mÃ´ hÃ¬nh khÃ´ng á»•n Ä‘á»‹nh, má»™t sá»‘ chu ká»³ há»c cÃ³ sá»± dao Ä‘á»™ng máº¡nh á»Ÿ tá»‰ lá»‡ nháº­n dáº¡ng Ä‘Ãºng hay nÃ³i cÃ¡ch khÃ¡c lÃ  thuáº­t toÃ¡n khÃ´ng Ä‘Æ°á»£c tá»‘i Æ°u vÃ  ngÆ°á»£c láº¡i Ä‘á»‘i vá»›i tá»‰ lá»‡ há»c nhá».

#### 2.1.3 Yáº¿u tá»‘ Ä‘Ã¡nh giÃ¡ má»™t thuáº­t toÃ¡n tá»‘i Æ°u

Má»™t vÃ i cÃ¡c yáº¿u tá»‘ hay Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»™t thuáº­t toÃ¡n tá»‘i Æ°u (Optimizer):

â€¢	Há»™i tá»¥ nhanh (trong quÃ¡ trÃ¬nh train)

â€¢	Sá»± tá»•ng quÃ¡t hÃ³a cao (váº«n nháº­n dáº¡ng Ä‘Æ°á»£c nhá»¯ng máº«u chÆ°a tá»«ng Ä‘Æ°á»£c huáº¥n luyá»‡n)

â€¢	Äá»™ chÃ­nh xÃ¡c cao

### 2.2 Má»™t sá»‘ thuáº­t toÃ¡n tá»‘i Æ°u (Optimization Algorithms)

Má»™t sá»‘ thuáº­t toÃ¡n tá»‘i Æ°u phá»• biáº¿n:

1.	Gradient Descent
2.	SGD vá»›i Ä‘á»™ng lÆ°á»£ng
3.	RMSProp
4.	Adagrad
5.	Adadelta
6.	Adam
7.	AdamW
8.	AMSGrad
   
#### 2.2.1 Gradient Descent (GD)

Gradient Descent (GD) lÃ  thuáº­t toÃ¡n tÃ¬m tá»‘i Æ°u chung cho cÃ¡c hÃ m sá»‘. Ã tÆ°á»Ÿng chung cá»§a GD lÃ  Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ Ä‘á»ƒ láº·p Ä‘i láº·p láº¡i thÃ´ng qua má»—i dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ giáº£m thiá»ƒu hÃ m chi phÃ­. 

ğ‘¤(ğ‘˜+1) = ğ‘¤(ğ‘˜) âˆ’ ğœ‚ âˆ‡ğ‘¤ ğ½(ğ‘¤(ğ‘˜) )

Trong Ä‘Ã³:

â€¢	ğ‘¤(ğ‘˜) : tham sá»‘ táº¡i bÆ°á»›c cáº­p nháº­t táº¡i lá»›p k

â€¢	Î—: tá»‰ lá»‡ há»c

â€¢	ğ½(ğ‘¤): hÃ m lá»—i

â€¢	âˆ‡ğ‘¤ ğ½(ğ‘¤ (ğ‘˜)): Ä‘áº¡o hÃ m cá»§a hÃ m lá»—i táº¡i Ä‘iá»ƒm ğ‘¤(ğ‘˜)

VÃ­ dá»¥:

```sh
from __future__ import division, print_function, unicode_literals
import math
import numpy as np 
import matplotlib.pyplot as plt

def grad(x):
    return 2*x+ 5*np.cos(x)

def cost(x):
    return x**2 + 5*np.sin(x)

def myGD1(eta, x0):
    x = [x0]
    for it in range(100):
        x_new = x[-1] - eta*grad(x[-1])
        if abs(grad(x_new)) < 1e-3:
            break
        x.append(x_new)
    return (x, it)

(x1, it1) = myGD1(.1, -5)
(x2, it2) = myGD1(.1, 5)
print('Solution x1 = %f, cost = %f, obtained after %d iterations'%(x1[-1], cost(x1[-1]), it1))
print('Solution x2 = %f, cost = %f, obtained after %d iterations'%(x2[-1], cost(x2[-1]), it2))
```
Káº¿t quáº£:

```sh
Solution x1 = -1.110667, cost = -3.246394, obtained after 11 iterations
Solution x2 = -1.110341, cost = -3.246394, obtained after 29 iterations
```
ï¶	Äiá»ƒm khá»Ÿi táº¡o khÃ¡c nhau

Sau khi cÃ³ cÃ¡c hÃ m cáº§n thiáº¿t, tÃ´i thá»­ tÃ¬m nghiá»‡m vá»›i cÃ¡c Ä‘iá»ƒm khá»Ÿi táº¡o khÃ¡c nhau lÃ  x0 = âˆ’5 vÃ  x0 = 5.

<img src="picture/2.2.png"> <img src="picture/2.2.1.png">

HÃ¬nh 2.2 Minh há»a thuáº­t toÃ¡n GD vá»›i Ä‘iá»ƒm khá»Ÿi táº¡o khÃ¡c nhau

Tá»« hÃ¬nh minh há»a trÃªn ta tháº¥y ráº±ng á»Ÿ hÃ¬nh bÃªn trÃ¡i, tÆ°Æ¡ng á»©ng vá»›i x0 =âˆ’5, nghiá»‡m há»™i tá»¥ nhanh hÆ¡n, vÃ¬ Ä‘iá»ƒm ban Ä‘áº§u x0 gáº§n vá»›i nghiá»‡m x* â‰ˆ âˆ’1 hÆ¡n. HÆ¡n ná»¯a, vá»›i x0 =5 á»Ÿ hÃ¬nh bÃªn pháº£i, Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m cÃ³ chá»©a má»™t khu vá»±c cÃ³ Ä‘áº¡o hÃ m khÃ¡ nhá» gáº§n Ä‘iá»ƒm cÃ³ hoÃ nh Ä‘á»™ báº±ng 2. 

=>	Äiá»u nÃ y khiáº¿n cho thuáº­t toÃ¡n la cÃ  á»Ÿ Ä‘Ã¢y khÃ¡ lÃ¢u. Khi vÆ°á»£t qua Ä‘Æ°á»£c Ä‘iá»ƒm nÃ y thÃ¬ má»i viá»‡c diá»…n ra ráº¥t tá»‘t Ä‘áº¹p.

ï¶	Learning rate khÃ¡c nhau

Tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a GD khÃ´ng nhá»¯ng phá»¥ thuá»™c vÃ o Ä‘iá»ƒm khá»Ÿi táº¡o ban Ä‘áº§u mÃ  cÃ²n phá»¥ thuá»™c vÃ o learning rate. 

VÃ­ dá»¥ vá»›i cÃ¹ng Ä‘iá»ƒm khá»Ÿi táº¡o x0 = âˆ’5 nhÆ°ng learning rate khÃ¡c nhau:

<img src="picture/2.3.png"> <img src="picture/2.3.1.png">

HÃ¬nh 2.3 Minh há»a thuáº­t toÃ¡n GD vá»›i Learning rate khÃ¡c nhau

Ta quan sÃ¡t tháº¥y hai Ä‘iá»u:

1.	Vá»›i learning rate nhá» Î·=0.01, tá»‘c Ä‘á»™ há»™i tá»¥ ráº¥t cháº­m. Trong vÃ­ dá»¥, do chá»n tá»‘i Ä‘a 100 vÃ²ng láº·p nÃªn thuáº­t toÃ¡n dá»«ng láº¡i trÆ°á»›c khi tá»›i Ä‘Ã­ch, máº·c dÃ¹ Ä‘Ã£ ráº¥t gáº§n. Trong thá»±c táº¿, khi viá»‡c tÃ­nh toÃ¡n trá»Ÿ nÃªn phá»©c táº¡p, learning rate quÃ¡ tháº¥p sáº½ áº£nh hÆ°á»Ÿng tá»›i tá»‘c Ä‘á»™ cá»§a thuáº­t toÃ¡n ráº¥t nhiá»u, tháº­m chÃ­ khÃ´ng bao giá» tá»›i Ä‘Æ°á»£c Ä‘Ã­ch.
2.	Vá»›i learning rate lá»›n Î·=0.5, thuáº­t toÃ¡n tiáº¿n ráº¥t nhanh tá»›i gáº§n Ä‘Ã­ch sau vÃ i vÃ²ng láº·p. Tuy nhiÃªn, thuáº­t toÃ¡n khÃ´ng há»™i tá»¥ Ä‘Æ°á»£c vÃ¬ bÆ°á»›c nháº£y quÃ¡ lá»›n, khiáº¿n nÃ³ cá»© quáº©n quanh á»Ÿ Ä‘Ã­ch.
   
ïƒ˜	Viá»‡c lá»±a chá»n learning rate ráº¥t quan trá»ng trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿. Viá»‡c lá»±a chá»n giÃ¡ trá»‹ nÃ y phá»¥ thuá»™c nhiá»u vÃ o tá»«ng bÃ i toÃ¡n vÃ  pháº£i lÃ m má»™t vÃ i thÃ­ nghiá»‡m Ä‘á»ƒ chá»n ra giÃ¡ trá»‹ tá»‘t nháº¥t. NgoÃ i ra, tÃ¹y vÃ o má»™t sá»‘ bÃ i toÃ¡n, GD cÃ³ thá»ƒ lÃ m viá»‡c hiá»‡u quáº£ hÆ¡n báº±ng cÃ¡ch chá»n ra learning rate phÃ¹ há»£p hoáº·c chá»n learning rate khÃ¡c nhau á»Ÿ má»—i vÃ²ng láº·p.

CÃ³ má»™t sá»‘ biáº¿n thá»ƒ khÃ¡c nhau cá»§a GD tÃ¹y thuá»™c vÃ o sá»‘ lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t. Gá»“m: 

1.	Batch Gradient Descent (Batch GD)
2.	Stochastic Gradient Descent (SGD)
3.	Mini-batch Gradient Descent (Mini-batch GD)
   
##### 2.2.1.1 Batch Gradient Descent (Batch GD)

Thuáº­t toÃ¡n Batch Gradient Descent (Batch GD) tÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t táº¡i w trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u. Táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh gradient trÆ°á»›c khi cáº­p nháº­t bá»™ trá»ng sá»‘ w. Háº¡n cháº¿ cá»§a Batch GD lÃ  khi táº­p dá»¯ liá»‡u lá»›n, viá»‡c tÃ­nh gradient sáº½ tá»‘n nhiá»u thá»i gian vÃ  chi phÃ­ tÃ­nh toÃ¡n.

##### 2.2.1.2 Stochastic Gradient Descent (SGD)

Äá»ƒ kháº¯c phá»¥c háº¡n cháº¿ cá»§a Bathc GD, thuáº­t toÃ¡n Stochastic Gradient Descent (SGD) thá»±c hiá»‡n viá»‡c cáº­p nháº­t trá»ng sá»‘ vá»›i má»—i máº«u dá»¯ liá»‡u x(i) cÃ³ nhÃ£n tÆ°Æ¡ng á»©ng y(i) nhÆ° sau: 

ğ‘¤(ğ‘˜+1) = ğ‘¤(ğ‘˜) âˆ’ ğœ‚ âˆ‡ğ‘¤ ğ½(ğ‘¤(ğ‘˜), x(i), y(i))

Vá»›i cÃ¡ch cáº­p nháº­t nÃ y, SGD thÆ°á»ng nhanh hÆ¡n Batch GD vÃ  cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ há»c trá»±c tuyáº¿n (online learning) khi táº­p dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c. 

Vá»›i SGD, bá»™ trá»ng sá»‘ w Ä‘Æ°á»£c cáº­p nháº­t thÆ°á»ng xuyÃªn hÆ¡n so vá»›i Batch GD vÃ  vÃ¬ váº­y hÃ m máº¥t mÃ¡t cÅ©ng dao Ä‘á»™ng nhiá»u hÆ¡n. Sá»± dao Ä‘á»™ng nÃ y khiáº¿n SGD cÃ³ váº» khÃ´ng á»•n Ä‘á»‹nh nhÆ°ng láº¡i cÃ³ Ä‘iá»ƒm tÃ­ch cá»±c lÃ  nÃ³ giÃºp di chuyá»ƒn Ä‘áº¿n nhá»¯ng Ä‘iá»ƒm cá»±c tiá»ƒu (Ä‘á»‹a phÆ°Æ¡ng) má»›i cÃ³ tiá»m nÄƒng hÆ¡n. Vá»›i tá»‘c Ä‘á»™ há»c giáº£m, kháº£ nÄƒng há»™i tá»¥ cá»§a SGD cÅ©ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Batch GD.

HÃ m sá»‘ trong Python Ä‘á»ƒ giáº£i Linear Regression theo SGD:

```sh
def sgrad(w, i, rd_id):
    true_i = rd_id[i]
    xi = Xbar[true_i, :]
    yi = y[true_i]
    a = np.dot(xi, w) - yi
    return (xi*a).reshape(2, 1)

def SGD(w_init, grad, eta):
    w = [w_init]
    w_last_check = w_init
    iter_check_w = 10
    N = X.shape[0]
    count = 0
    for it in range(10):  
# shuffle data 
        rd_id = np.random.permutation(N)
        for i in range(N):
            count += 1 
            g = sgrad(w[-1], i, rd_id)
            w_new = w[-1] - eta*g
            w.append(w_new)
            if count%iter_check_w == 0:
                w_this_check = w_new                 
                if np.linalg.norm(w_this_check - w_last_check)/len(w_init) < 1e-3:                                    
                    return w
                w_last_check = w_this_check
    return w
```

Káº¿t quáº£ thu Ä‘Æ°á»£c:

<img src="picture/2.4.png"> <img src="picture/2.4.1.png">
HÃ¬nh 2.4 TrÃ¡i: Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m vá»›i SGD. Pháº£i: giÃ¡ trá»‹ cá»§a Loss function táº¡i 50 vÃ²ng láº·p Ä‘áº§u tiÃªn.

##### 2.2.1.3 Mini-batch Gradient Descent (Mini-batch GD)

CÃ¡ch tiáº¿p cáº­n thá»© ba lÃ  thuáº­t toÃ¡n Mini-batch Gradient Descent (Mini-batch GD). KhÃ¡c vá»›i hai thuáº­t toÃ¡n trÆ°á»›c, Mini-batch GD sá»­ dá»¥ng t Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»ƒ cáº­p nháº­t bá»™ trá»ng sá»‘ (1<t<N) vá»›i N lÃ  tá»•ng sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u). 

ğ‘¤(ğ‘˜+1) = ğ‘¤(ğ‘˜) âˆ’ ğœ‚ âˆ‡ğ‘¤ ğ½(ğ‘¤(ğ‘˜), x(i: i+t), y(i: i+t))

Vá»›i x(i: i+t) Ä‘Æ°á»£c hiá»ƒu lÃ  dá»¯ liá»‡u tá»« thá»© i tá»›i thá»© i+tâˆ’1. Dá»¯ liá»‡u nÃ y sau má»—i epoch lÃ  khÃ¡c nhau vÃ¬ chÃºng cáº§n Ä‘Æ°á»£c xÃ¡o trá»™n. Má»™t láº§n ná»¯a, cÃ¡c thuáº­t toÃ¡n khÃ¡c cho GD nhÆ° Momentum, Adagrad, Adadelta, â€¦ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o Ä‘Ã¢y. GiÃ¡ trá»‹ t thÆ°á»ng Ä‘Æ°á»£c chá»n lÃ  khoáº£ng tá»« 50 Ä‘áº¿n 100. 

Mini-batch GD giáº£m sá»± dao Ä‘á»™ng cá»§a hÃ m máº¥t mÃ¡t so vá»›i SGD vÃ  chi phÃ­ tÃ­nh gradient vá»›i k Ä‘iá»ƒm dá»¯ liá»‡u lÃ  cháº¥p nháº­n Ä‘Æ°á»£c. 

Mini-batch GD thÆ°á»ng Ä‘Æ°á»£c lá»±a chá»n khi huáº¥n luyá»‡n máº¡ng nÆ¡ron vÃ  vÃ¬ váº­y trong má»™t sá»‘ trÆ°á»ng há»£p, SGD Ä‘Æ°á»£c hiá»ƒu lÃ  Mini-batch GD. RiÃªng báº£n thÃ¢n Mini-batch GD khÃ´ng Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m máº¥t mÃ¡t mÃ  bÃªn cáº¡nh Ä‘Ã³ cÃ¡c yáº¿u tá»‘ nhÆ° tá»‘c Ä‘á»™ há»c, thuá»™c tÃ­nh dá»¯ liá»‡u vÃ  tÃ­nh cháº¥t cá»§a hÃ m máº¥t mÃ¡t cÅ©ng áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘iá»u nÃ y.

VÃ­ dá»¥ vá» giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t má»—i khi cáº­p nháº­t tham sá»‘ w cá»§a má»™t bÃ i toÃ¡n khÃ¡c:

<img src="picture/2.5.png">
HÃ¬nh 2.5 VÃ­ dá»¥ vá» Mini-batch Gradient Descent

ïƒ˜	HÃ m máº¥t mÃ¡t nháº£y lÃªn nháº£y xuá»‘ng (fluctuate) sau má»—i láº§n cáº­p nháº­t nhÆ°ng nhÃ¬n chung giáº£m dáº§n vÃ  cÃ³ xu hÆ°á»›ng há»™i tá»¥ vá» cuá»‘i.

#### 2.2.2 SGD vá»›i Ä‘á»™ng lÆ°á»£ng (SGD with momentum)

SGD vá»›i momentum lÃ  phÆ°Æ¡ng phÃ¡p giÃºp tÄƒng tá»‘c cÃ¡c vectÆ¡ Ä‘á»™ dá»‘c theo Ä‘Ãºng hÆ°á»›ng, vÃ  giÃºp há»‡ thá»‘ng há»™i tá»¥ nhanh hÆ¡n. ÄÃ¢y lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a phá»• biáº¿n nháº¥t vÃ  nhiá»u mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i sá»­ dá»¥ng nÃ³ Ä‘á»ƒ Ä‘Ã o táº¡o. 

MÃ´ táº£ nhÆ° sau: 

ğ‘£ğ‘— â† ğ›¼ âˆ— ğ‘£ğ‘— âˆ’ ğœ‚ âˆ— ğ›»ğ‘Š âˆ‘_1^mâ–’ã€–L_m (w) ã€— 

ğ‘¤ğ‘— â† ğ‘£ğ‘— + ğ‘¤ğ‘—

PhÆ°Æ¡ng trÃ¬nh cÃ³ hai pháº§n. Trong Ä‘Ã³:

	vj: Ä‘á»™ dá»‘c Ä‘Æ°á»£c giá»¯ láº¡i tá»« cÃ¡c láº§n láº·p trÆ°á»›c
 
	Há»‡ sá»‘ Ä‘á»™ng lÆ°á»£ng Î±: tá»‰ lá»‡ pháº§n trÄƒm cá»§a Ä‘á»™ dá»‘c Ä‘Æ°á»£c giá»¯ láº¡i má»—i láº§n láº·p
 
	L: hÃ m máº¥t mÃ¡t
 
	Î·: tá»‰ lá»‡ há»c

#### 2.2.3 RMSProp (Root Mean Square Propogation)

RMSProp sá»­ dá»¥ng trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng cá»§a gradient Ä‘á»ƒ chuáº©n hÃ³a gradient. CÃ³ tÃ¡c dá»¥ng cÃ¢n báº±ng kÃ­ch thÆ°á»›c bÆ°á»›c - giáº£m bÆ°á»›c cho Ä‘á»™ dá»‘c lá»›n Ä‘á»ƒ trÃ¡nh hiá»‡n tÆ°á»£ng phÃ¡t ná»• Ä‘á»™ dá»‘c (Exploding Gradient), vÃ  tÄƒng bÆ°á»›c cho Ä‘á»™ dá»‘c nhá» Ä‘á»ƒ trÃ¡nh biáº¿n máº¥t Ä‘á»™ dá»‘c (Vanishing Gradient). RMSProp tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c táº­p, vÃ  chá»n má»™t tá»‰ lá»‡ há»c táº­p khÃ¡c nhau cho má»—i tham sá»‘. 

PhÆ°Æ¡ng phÃ¡p cáº­p nháº­t cÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° mÃ´ táº£:

ğ‘ ğ‘¡ = ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ ğœŒ) âˆ— g_t^2

ğ›¥ğ‘¥ğ‘¡ = -Î·/âˆš(s_t  + Ïµ) âˆ— ğ‘”ğ‘¡ 

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ + ğ›¥ğ‘¥t

Trong Ä‘Ã³:

	ğ‘ ğ‘¡: tÃ­ch luá»¹ phÆ°Æ¡ng sai cá»§a cÃ¡c gradient trong quÃ¡ khá»©
 
	ğœŒ: tham sá»‘ suy giáº£m
 
	ğ›¥ğ‘¥ğ‘¡: sá»± thay Ä‘á»•i cÃ¡c tham sá»‘ trong mÃ´ hÃ¬nh
 
	ğ‘”ğ‘¡: gradient cá»§a cÃ¡c tham sá»‘ táº¡i vÃ²ng láº·p t
 
	Ïµ: tham sá»‘ Ä‘áº£m báº£o káº¿t quáº£ xáº¥p xá»‰ cÃ³ Ã½ nghÄ©a.
 
#### 2.2.4 Adagrad

Adagrad lÃ  má»™t ká»¹ thuáº­t há»c mÃ¡y tiÃªn tiáº¿n, thá»±c hiá»‡n giáº£m dáº§n Ä‘á»™ dá»‘c báº±ng cÃ¡ch thay Ä‘á»•i tá»‘c Ä‘á»™ há»c táº­p. Adagrad Ä‘Æ°á»£c cáº£i thiá»‡n hÆ¡n báº±ng cÃ¡ch cho trá»ng sá»‘ há»c táº­p chÃ­nh xÃ¡c dá»±a vÃ o Ä‘áº§u vÃ o trÆ°á»›c nÃ³ Ä‘á»ƒ tá»± Ä‘iá»u chá»‰nh tá»‰ lá»‡ há»c theo hÆ°á»›ng tá»‘i Æ°u nháº¥t thay vÃ¬ vá»›i má»™t tá»‰ lá»‡ há»c duy nháº¥t cho táº¥t cáº£ cÃ¡c nÃºt.

Thuáº­t toÃ¡n Adagrad Ä‘Æ°á»£c Duchi J. vÃ  cÃ¡c cá»™ng sá»± Ä‘á» xuáº¥t nÄƒm 2011. KhÃ¡c vá»›i SGD, tá»‘c Ä‘á»™ há»c trong Adagrad thay Ä‘á»•i tÃ¹y thuá»™c vÃ o trá»ng sá»‘: tá»‘c Ä‘á»™ há»c tháº¥p Ä‘á»‘i vá»›i cÃ¡c trá»ng sá»‘ tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘áº·c trÆ°ng phá»• biáº¿n, tá»‘c Ä‘á»™ há»c cao Ä‘á»‘i vá»›i cÃ¡c trá»ng sá»‘ tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘áº·c trÆ°ng Ã­t phá»• biáº¿n.

gt, i  = âˆ‡wJ (wt, i)

Trong Ä‘Ã³:

	gt: gradient cá»§a hÃ m máº¥t mÃ¡t táº¡i bÆ°á»›c t
 
	gt, i :  Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m máº¥t mÃ¡t theo wi táº¡i bÆ°á»›c t
 
Quy táº¯c cáº­p nháº­t cá»§a Adagrad:

wğ‘¡+1, i = wt, i -Î·/âˆš(G_(t,ii)  + Ïµ) âˆ— ğ‘”ğ‘¡, i

Theo quy táº¯c cáº­p nháº­t, Adagrad Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c Î· táº¡i bÆ°á»›c t tÆ°Æ¡ng á»©ng vá»›i trá»ng sá»‘ wi xÃ¡c Ä‘á»‹nh dá»±a trÃªn cÃ¡c gradient Ä‘Ã£ tÃ­nh Ä‘Æ°á»£c theo wi. 

	Máº«u sá»‘ lÃ  chuáº©n L2 (L2 norm) cá»§a ma tráº­n Ä‘Æ°á»ng chÃ©o Gt trong Ä‘Ã³ pháº§n tá»­ i,i lÃ  tá»•ng bÃ¬nh phÆ°Æ¡ng cá»§a cÃ¡c gradient tÆ°Æ¡ng á»©ng vá»›i wi tÃ­nh Ä‘áº¿n bÆ°á»›c t.
 
	Îµ lÃ  má»™t sá»‘ dÆ°Æ¡ng khÃ¡ nhá» nháº±m trÃ¡nh trÆ°á»ng há»£p máº«u sá»‘ báº±ng 0.
 
Quy táº¯c cáº­p nháº­t trÃªn cÃ³ thá»ƒ viáº¿t dÆ°á»›i dáº¡ng tá»•ng quÃ¡t hÆ¡n nhÆ° sau:

wğ‘¡+1 = wt -Î·/âˆš(G_t  + Ïµ) â¨€ğ‘”ğ‘¡

Trong Ä‘Ã³, â¨€ lÃ  phÃ©p nhÃ¢n ma tráº­n-vectÆ¡ giá»¯a Gt vÃ  gt . 

CÃ³ thá»ƒ nháº­n tháº¥y ráº±ng trong thuáº­t toÃ¡n Adagrad tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh. Adagrad thÆ°á»ng khÃ¡ hiá»‡u quáº£ Ä‘á»‘i vá»›i bÃ i toÃ¡n cÃ³ dá»¯ liá»‡u phÃ¢n máº£nh. Tuy nhiÃªn, háº¡n cháº¿ cá»§a Adagrad lÃ  cÃ¡c tá»•ng bÃ¬nh phÆ°Æ¡ng á»Ÿ máº«u sá»‘ ngÃ y cÃ ng lá»›n khiáº¿n tá»‘c Ä‘á»™ há»c ngÃ y cÃ ng giáº£m vÃ  cÃ³ thá»ƒ tiá»‡m cáº­n Ä‘áº¿n giÃ¡ trá»‹ 0 khiáº¿n cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n gáº§n nhÆ° Ä‘Ã³ng bÄƒng. BÃªn cáº¡nh Ä‘Ã³, giÃ¡ trá»‹ tá»‘c Ä‘á»™ há»c Î· cÅ©ng pháº£i Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh má»™t cÃ¡ch thá»§ cÃ´ng.

#### 2.2.5 Adadelta

Thuáº­t toÃ¡n Adadelta Ä‘Æ°á»£c Zeiler vÃ  cÃ¡c cá»™ng sá»± Ä‘á» xuáº¥t nÄƒm 2012. Adadelta lÃ  má»™t biáº¿n thá»ƒ cá»§a Adagrad Ä‘á»ƒ kháº¯c phá»¥c tÃ¬nh tráº¡ng giáº£m tá»‘c Ä‘á»™ há»c á»Ÿ Adagrad. Adadelta khÃ´ng cÃ³ tham sá»‘ tá»‰ lá»‡ há»c cho nÃªn, thay vÃ¬ lÆ°u láº¡i táº¥t cáº£ gradient nhÆ° Adagrad, Adadelta giá»›i háº¡n tÃ­ch lÅ©y gradient theo cá»­a sá»• cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh cá»§a trá»ng sá»‘ w. Báº±ng cÃ¡ch nÃ y, Adadelta váº«n tiáº¿p tá»¥c há»c sau nhiá»u bÆ°á»›c cáº­p nháº­t.

g_t^'= âˆš((ã€–Î”xã€—_(t-1)+Ïµ)/(s_t+Ïµ)) *ğ‘”ğ‘¡ 

ğ‘¥ğ‘¡ = ğ‘¥ğ‘¡âˆ’1 âˆ’ g_t^' 

ğ›¥ğ‘¥ğ‘¡ = ğœŒğ›¥ğ‘¥ğ‘¡âˆ’1 + (1 âˆ’ ğœŒ) x_t^2

Tá»« cÃ´ng thá»©c, Adadelta sá»­ dá»¥ng 2 biáº¿n tráº¡ng thÃ¡i: 

	ğ‘ ğ‘¡: Ä‘á»ƒ lÆ°u trá»¯ trung bÃ¬nh cá»§a khoáº£ng thá»i gian thá»© hai cá»§a gradient vÃ  Î”ğ‘¥ğ‘¡ Ä‘á»ƒ lÆ°u trá»¯ trung bÃ¬nh cá»§a khoáº£ng thá»i gian thá»© 2 cá»§a sá»± thay Ä‘á»•i cÃ¡c tham sá»‘ trong mÃ´ hÃ¬nh. 
 
	g_t^': cÄƒn báº­c hai thÆ°Æ¡ng cá»§a trung bÃ¬nh tá»‘c Ä‘á»™ thay Ä‘á»•i bÃ¬nh phÆ°Æ¡ng vÃ  trung bÃ¬nh mÃ´-men báº­c hai cá»§a gradient. 
 
#### 2.2.6 Adam

Adam Ä‘Æ°á»£c xem nhÆ° lÃ  sá»± káº¿t há»£p cá»§a RMSprop vÃ  Stochastic Gradient Descent vá»›i Ä‘á»™ng lÆ°á»£ng. Adam lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‰ lá»‡ há»c thÃ­ch á»©ng, nÃ³ tÃ­nh toÃ¡n tá»‰ lá»‡ há»c táº­p cÃ¡ nhÃ¢n cho cÃ¡c tham sá»‘ khÃ¡c nhau. Adam sá»­ dá»¥ng Æ°á»›c tÃ­nh cá»§a khoáº£ng thá»i gian thá»© nháº¥t vÃ  thá»© hai cá»§a Ä‘á»™ dá»‘c Ä‘á»ƒ Ä‘iá»u chá»‰nh tá»‰ lá»‡ há»c cho tá»«ng trá»ng sá»‘ cá»§a máº¡ng nÆ¡-ron.

Tuy nhiÃªn, qua nghiÃªn cá»©u thá»±c nghiá»‡m, trong má»™t sá»‘ trÆ°á»ng há»£p, Adam váº«n cÃ²n gáº·p pháº£i nhiá»u thiáº¿u sÃ³t so vá»›i thuáº­t toÃ¡n SGD. Thuáº­t toÃ¡n Adam Ä‘Æ°á»£c mÃ´ táº£: 

ğ‘šğ‘¡ = ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ ğ›½1)ğ‘”ğ‘¡

ğ‘£ğ‘¡ = ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ ğ›½2) g_t^2

Trong Ä‘Ã³:

	vt lÃ  trung bÃ¬nh Ä‘á»™ng cá»§a bÃ¬nh phÆ°Æ¡ng
 
	mt lÃ  trung bÃ¬nh Ä‘á»™ng cá»§a gradient
 
	Î²1vÃ  Î²2 lÃ  tá»‘c Ä‘á»™ cá»§a di chuyá»ƒn
 
#### 2.2.7 AdamW

AdamW lÃ  má»™t biáº¿n thá»ƒ cá»§a Adam. Ã tÆ°á»Ÿng cá»§a AdamW khÃ¡ Ä‘Æ¡n giáº£n: khi thá»±c hiá»‡n thuáº­t toÃ¡n Adam vá»›i L2 regularization (chuáº©n hÃ³a L2), tÃ¡c giáº£ loáº¡i bá» pháº§n tiÃªu biáº¿n cá»§a trá»ng sá»‘ (weight decay) wtÎ¸t khá»i cÃ´ng thá»©c tÃ­nh gradient hÃ m máº¥t mÃ¡t táº¡i thá»i Ä‘iá»ƒm t: 

gt = ğ›¥f(Î¸t) + wtÎ¸t

vÃ  thay vÃ o Ä‘Ã³, Ä‘Æ°a pháº§n giÃ¡ trá»‹ Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n tÃ¡ch nÃ y vÃ o quÃ¡ trÃ¬nh cáº­p nháº­t trá»ng sá»‘: 

Î¸_(t+1,i)=Î¸_(t,i)-Î·(1/âˆš((v_t ) Ì‚+Ïµ)*(m_t ) Ì‚+w_(t,i) Î¸_(t,i) ),âˆ€t

#### 2.2.8 AMSGrad

AMSGrad sá»­ dá»¥ng giÃ¡ trá»‹ lá»›n nháº¥t cá»§a cÃ¡c bÃ¬nh phÆ°Æ¡ng gradient trÆ°á»›c Ä‘Ã³ vt Ä‘á»ƒ cáº­p nháº­t cÃ¡c trá»ng sá»‘. á» Ä‘Ã¢y, vt cÅ©ng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° trong thuáº­t toÃ¡n Adam: 

ğ‘£ğ‘¡ = ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ ğ›½2) g_t^2

Thay vÃ¬ trá»±c tiáº¿p sá»­ dá»¥ng vt (hay giÃ¡ trá»‹ Æ°á»›c lÆ°á»£ng (v_t ) Ì‚ ), thuáº­t toÃ¡n sáº½ sá»­ dá»¥ng giÃ¡ trá»‹ trÆ°á»›c Ä‘Ã³ vt-1 náº¿u giÃ¡ trá»‹ nÃ y lá»›n hÆ¡n giÃ¡ trá»‹ hiá»‡n táº¡i: 

(v_t ) Ì‚=maxâ¡((v_(t-1) ) Ì‚,v_t) 

TÆ°Æ¡ng tá»± nhÆ° trong thuáº­t toÃ¡n Adam, cÃ¡c giÃ¡ trá»‹ Ä‘Æ°á»£c Æ°á»›c lÆ°á»£ng theo cÃ´ng thá»©c dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ khá»­ lá»‡ch cho cÃ¡c trá»ng sá»‘:

ğ‘šğ‘¡ = ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ ğ›½1)ğ‘”ğ‘¡

ğ‘£ğ‘¡ = ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ ğ›½2) g_t^2

(v_t ) Ì‚=maxâ¡((v_(t-1) ) Ì‚,v_t) 

AMSGrad Ä‘Æ°á»£c cáº­p nháº­t theo quy táº¯c:

wğ‘¡+1 = wt -Î·/âˆš((v_t ) Ì‚  + Ïµ)*mğ‘¡

### 2.3 So sÃ¡nh cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u vá»›i bá»™ dá»¯ liá»‡u MNIST vÃ  CIFAR-10
#### 2.3.1 CÆ¡ sá»Ÿ dá»¯ liá»‡u
##### 2.3.1.1 MNIST

Bá»™ dá»¯ liá»‡u MNIST lÃ  bá»™ dá»¯ liá»‡u gá»“m cÃ¡c hÃ¬nh áº£nh xÃ¡m (grayscale picture) cÃ¡c chá»¯ sá»‘ viáº¿t tay Ä‘Æ°á»£c chia sáº» bá»Ÿi Yann Lecun bao gá»“m 70000 áº£nh chá»¯ sá»‘ viáº¿t tay Ä‘Æ°á»£c chia thÃ nh 2 táº­p: táº­p huáº¥n luyá»‡n gá»“m 60000 áº£nh vÃ  táº­p kiá»ƒm tra 10000 áº£nh. CÃ¡c chá»¯ sá»‘ viáº¿t tay á»Ÿ táº­p MNIST Ä‘Æ°á»£c chia thÃ nh 10 nhÃ³m tÆ°Æ¡ng á»©ng vá»›i cÃ¡c chá»¯ sá»‘ tá»« 0 Ä‘áº¿n 9. Táº¥t cáº£ hÃ¬nh áº£nh trong táº­p MNIST Ä‘á»u Ä‘Æ°á»£c chuáº©n hÃ³a vá»›i kÃ­ch thÆ°á»›c 28 x 28 Ä‘iá»ƒm áº£nh. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ hÃ¬nh áº£nh Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« bá»™ dá»¯ liá»‡u.

<img src="picture/2.6.png">

HÃ¬nh 2.6 HÃ¬nh áº£nh chá»¯ sá»‘ viáº¿t tay tá»« táº­p MNIST

(https://vi.wikipedia.org/wiki/C%C6%A1_s%E1%BB%9F_d%E1%BB%AF_li%E1%BB%87u_MNIST)

###### 2.3.1.2 CIFAR-10

Bá»™ cÆ¡ sá»Ÿ dá»¯ liá»‡u CIFAR-10 lÃ  bá»™ dá»¯ liá»‡u chá»©a cÃ¡c áº£nh mÃ u cÃ³ kÃ­ch thÆ°á»›c 32 x 32 x 3 (3 lá»›p mÃ u RGB) trong 10 nhÃ³m khÃ¡c nhau, gá»“m: mÃ¡y bay, Ã´ tÃ´, chim, mÃ¨o, hÆ°Æ¡u, chÃ³, áº¿ch, ngá»±a, tÃ u vÃ  xe táº£i. Má»—i nhÃ³m gá»“m 6000 hÃ¬nh áº£nh, cÃ¹ng vá»›i sá»± Ä‘a dáº¡ng vá» cÃ¡c thÃ nh pháº§n nhÆ° Ä‘á»™ sÃ¡ng, vá»‹ trÃ­, hÆ°á»›ng cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng. NÃ³ lÃ  má»™t trong nhá»¯ng bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t cho nghiÃªn cá»©u mÃ¡y há»c bao gá»“m 60000 áº£nh Ä‘Æ°á»£c chia thÃ nh 2 táº­p: táº­p huáº¥n luyá»‡n gá»“m 50000 áº£nh vÃ  táº­p kiá»ƒm tra 10000 áº£nh.

<img src="picture/2.7.png">

HÃ¬nh 2.7 Má»™t sá»‘ hÃ¬nh áº£nh tá»« bá»™ dá»¯ liá»‡u chá»©a áº£nh CIFAR-10

(https://www.cs.toronto.edu/~kriz/cifar.html)

###### 2.3.1.3 CINIC-10

Bá»™ cÆ¡ sá»Ÿ dá»¯ liá»‡u CINIC-10 gá»“m 270,000 bá»©c áº£nh, thuá»™c vá» 10 lá»›p khÃ¡c nhau nhÆ° á»Ÿ CIFAR-10, chia lÃ m 3 pháº§n: táº­p huáº¥n luyá»‡n, táº­p kiá»ƒm thá»­ vÃ  táº­p kiá»ƒm Ä‘á»‹nh, má»—i táº­p cÃ³ 90,000 pháº§n tá»­. CINIC cÃ³ thá»ƒ coi lÃ  táº­p má»Ÿ rá»™ng cá»§a CIFAR-10, bá»• sung thÃªm nhiá»u pháº§n tá»­ áº£nh trÃ­ch xuáº¥t tá»« táº­p ImageNet vÃ  Ä‘Æ°á»£c chá»‰nh sá»­a Ä‘á»ƒ cÃ³ kÃ­ch thÆ°á»›c tÆ°Æ¡ng tá»± vá»›i pháº§n tá»­ áº£nh trong táº­p CIFAR-10. 
CINIC-10 cÃ³ táº­p kiá»ƒm thá»­ lÃªn Ä‘áº¿n 90,000 pháº§n tá»­. Theo [5], viá»‡c cÃ¡c máº«u dá»¯ liá»‡u trÃ­ch xuáº¥t tá»« ImageNet bá»‹ giáº£m kÃ­ch thÆ°á»›c thÃ nh 32x32 sáº½ lÃ m tÄƒng Ä‘á»™ khÃ³ cá»§a viá»‡c phÃ¢n lá»›p do sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng Ã­t hÆ¡n. Viá»‡c táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm thá»­ cÃ³ tá»‰ lá»‡ 1:1 cÅ©ng sáº½ giÃºp Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh.

<img src="picture/2.8.png">

HÃ¬nh 2.8 Má»™t sá»‘ hÃ¬nh áº£nh tá»« bá»™ dá»¯ liá»‡u chá»©a áº£nh CINIC-10

(https://paperswithcode.com/dataset/cinic-10) 

#### 2.3.2 Káº¿t quáº£

##### 2.3.2.1 Káº¿t quáº£ vá»›i bá»™ dá»¯ liá»‡u MNIST

<img src="<img src="picture/2.9.png">">

HÃ¬nh 2.9 Tá»‰ lá»‡ máº¥t mÃ¡t cá»§a cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u trÃªn táº­p dá»¯ liá»‡u MNIST

<img src="picture/2.10.png">

HÃ¬nh 2.10 Tá»‰ lá»‡ nháº­n dáº¡ng Ä‘Ãºng cá»§a cÃ¡c thuáº­t toÃ¡n trÃªn táº­p huáº¥n luyá»‡n vÃ  táº­p Ä‘Ã¡nh giÃ¡ cá»§a táº­p dá»¯ liá»‡u MNIST

##### 2.3.2.2 Káº¿t quáº£ vá»›i bá»™ dá»¯ liá»‡u CIFAR-10

<img src="picture/2.11.png">

HÃ¬nh 2.11 Tá»‰ lá»‡ máº¥t mÃ¡t cá»§a cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u trÃªn táº­p dá»¯ liá»‡u CIFAR-10

<img src="picture/2.12.png">

HÃ¬nh 2.12 Tá»‰ lá»‡ nháº­n dáº¡ng Ä‘Ãºng cá»§a cÃ¡c thuáº­t toÃ¡n trÃªn táº­p huáº¥n luyá»‡n vÃ  táº­p Ä‘Ã¡nh giÃ¡ cá»§a táº­p dá»¯ liá»‡u CIFAR-10

##### 2.3.2.3 Káº¿t quáº£ vá»›i bá»™ dá»¯ liá»‡u CINIC-10

<img src="picture/2.13.png">

HÃ¬nh 2.13 Äá»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh ResNet110 huáº¥n luyá»‡n trÃªn táº­p CINIC-10

<img src="picture/2.14.png">

HÃ¬nh 2.14 Káº¿t quáº£ thá»­ nghiá»‡m vá»›i táº­p CINIC-10

(cÃ¡c giÃ¡ trá»‹ in Ä‘áº­m lÃ  cÃ¡c káº¿t quáº£ tá»‘t nháº¥t cá»§a má»—i cá»™t)

## CHÆ¯Æ NG 3 â€“ CONTINUAL LEARNING VÃ€ TEST PRODUCTION

### 3.1 Continual Learning

#### 3.1.1 Continual Learning lÃ  gÃ¬?

<img src="picture/3.1.png">

HÃ¬nh 3.1 Continual Learning

Continual Learning (cÃ²n gá»i lÃ  Incremental Learning, Life-long Learning) lÃ  lÃ  má»™t mÃ´ hÃ¬nh há»c mÃ¡y táº­p trung vÃ o cÃ¡c mÃ´ hÃ¬nh Ä‘Ã o táº¡o Ä‘á»ƒ tiáº¿p thu kiáº¿n thá»©c má»›i vÃ  thÃ­ch á»©ng vá»›i viá»‡c thay Ä‘á»•i dá»¯ liá»‡u theo thá»i gian.. NgÆ°á»£c láº¡i vá»›i há»c mÃ¡y truyá»n thá»‘ng, trong Ä‘Ã³ cÃ¡c mÃ´ hÃ¬nh thÆ°á»ng Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn cÃ¡c táº­p dá»¯ liá»‡u cá»‘ Ä‘á»‹nh vÃ  giáº£ Ä‘á»‹nh ráº±ng viá»‡c phÃ¢n phá»‘i dá»¯ liá»‡u khÃ´ng Ä‘á»•i, há»c liÃªn tá»¥c Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ cÃ¡c phÃ¢n phá»‘i dá»¯ liá»‡u Ä‘ang phÃ¡t triá»ƒn vÃ  liÃªn tá»¥c há»c há»i tá»« dá»¯ liá»‡u má»›i trong khi váº«n giá»¯ Ä‘Æ°á»£c kiáº¿n thá»©c tá»« nhá»¯ng tráº£i nghiá»‡m trÆ°á»›c Ä‘Ã³. Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng trong cÃ¡c trÆ°á»ng há»£p dá»¯ liá»‡u khÃ´ng cá»‘ Ä‘á»‹nh, nghÄ©a lÃ  nÃ³ thay Ä‘á»•i theo thá»i gian.

NgoÃ i ra, há»‡ thá»‘ng Continual Learning cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t thuáº­t toÃ¡n thÃ­ch á»©ng cÃ³ kháº£ nÄƒng há»c tá»« má»™t luá»“ng thÃ´ng tin liÃªn tá»¥c, vá»›i thÃ´ng tin Ä‘Ã³ sáº½ dáº§n dáº§n cÃ³ sáºµn theo thá»i gian vÃ  trong Ä‘Ã³ sá»‘ lÆ°á»£ng nhiá»‡m vá»¥ cáº§n há»c (VÃ­ dá»¥: cÃ¡c lá»›p thÃ nh viÃªn trong má»™t nhiá»‡m vá»¥ phÃ¢n loáº¡i) khÃ´ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c . Äiá»u quan trá»ng lÃ  viá»‡c cung cáº¥p thÃ´ng tin má»›i pháº£i diá»…n ra mÃ  khÃ´ng bá»‹ lÃ£ng quÃªn hoáº·c can thiá»‡p má»™t cÃ¡ch nghiÃªm trá»ng.

#### 3.1.2 CÃ¡c thÃ¡ch thá»©c chÃ­nh vÃ  giáº£i phÃ¡p liÃªn quan Ä‘áº¿n Continual Learning

##### 3.1.2.1 Sá»± quÃªn lÃ£ng nghiÃªm trá»ng (Catastrophic Forgetting)

Má»™t trong nhá»¯ng thÃ¡ch thá»©c chÃ­nh trong viá»‡c há»c táº­p liÃªn tá»¥c lÃ  ngÄƒn cháº·n tÃ¬nh tráº¡ng quÃªn lÃ£ng nghiÃªm trá»ng. Äiá»u nÃ y Ä‘á» cáº­p Ä‘áº¿n hiá»‡n tÆ°á»£ng má»™t mÃ´ hÃ¬nh quÃªn thÃ´ng tin Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³ khi Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u má»›i. Nhiá»u ká»¹ thuáº­t khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, cháº³ng háº¡n nhÆ° phÆ°Æ¡ng phÃ¡p chÃ­nh quy hÃ³a, bá»™ Ä‘á»‡m phÃ¡t láº¡i vÃ  phÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n kiáº¿n trÃºc nhÆ° lÃ  bá»™ nhá»› phÃ¢n Ä‘oáº¡n tháº§n kinh (neural episodic memories).

<img src="picture/3.2.png">

HÃ¬nh 3.2 TÃ³m táº¯t cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng Ä‘á»ƒ giáº£i quyáº¿t sá»± quÃªn lÃ£ng nghiÃªm trá»ng

1.	Bá»™ Ä‘á»‡m phÃ¡t láº¡i (Replay Buffers)

Bá»™ Ä‘á»‡m phÃ¡t láº¡i lÆ°u trá»¯ má»™t táº­p há»£p con dá»¯ liá»‡u trong quÃ¡ khá»© vÃ  sá»­ dá»¥ng nÃ³ trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o Ä‘á»ƒ giÃºp mÃ´ hÃ¬nh lÆ°u giá»¯ kiáº¿n thá»©c vá» cÃ¡c tÃ¡c vá»¥ trÆ°á»›c Ä‘Ã³. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh xem láº¡i vÃ  huáº¥n luyá»‡n dá»¯ liá»‡u cÅ© Ä‘á»ƒ giáº£m thiá»ƒu tÃ¬nh tráº¡ng quÃªn dá»¯ liá»‡u theo Ä‘á»‹nh ká»³.

2.	ChÃ­nh quy hÃ³a (Regularization)

CÃ¡c ká»¹ thuáº­t nhÆ° elastic weight consolidation (EWC) vÃ  synaptic intelligence (SI) Ä‘Æ°a ra cÃ¡c thuáº­t ngá»¯ chÃ­nh quy hÃ³a cho loss function , xá»­ pháº¡t cÃ¡c thay Ä‘á»•i Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ thiáº¿t yáº¿u quan trá»ng cho cÃ¡c tÃ¡c vá»¥ trÆ°á»›c Ä‘Ã¢y. Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh lÆ°u giá»¯ kiáº¿n thá»©c vá» cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³.

3.	PhÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n kiáº¿n trÃºc (Architectural Approaches)

Má»™t sá»‘ phÆ°Æ¡ng phÃ¡p liÃªn quan Ä‘áº¿n viá»‡c sá»­a Ä‘á»•i kiáº¿n trÃºc neural network Ä‘á»ƒ táº¡o Ä‘iá»u kiá»‡n cho viá»‡c há»c táº­p liÃªn tá»¥c. VÃ­ dá»¥: progressive neural networks (PNNs) tÄƒng dáº§n máº¡ng khi há»c Ä‘Æ°á»£c cÃ¡c nhiá»‡m vá»¥ má»›i, trong khi cÃ¡c máº¡ng khÃ¡c sá»­ dá»¥ng kiáº¿n trÃºc mÃ´-Ä‘un hoáº·c cÃ³ thá»ƒ má»Ÿ rá»™ng.

4.	Há»c chuyá»ƒn giao (Transfer Learning)

CÃ¡c ká»¹ thuáº­t  há»c chuyá»ƒn giao cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho phÃ¹ há»£p vá»›i cÃ¡c tÃ¬nh huá»‘ng há»c táº­p liÃªn tá»¥c khi má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c trÃªn má»™t táº­p dá»¯ liá»‡u lá»›n vÃ  sau Ä‘Ã³ tinh chá»‰nh cho má»™t nhiá»‡m vá»¥ má»›i. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c vá» dá»¯ liá»‡u Ä‘a dáº¡ng cÃ³ thá»ƒ khÃ¡i quÃ¡t hÃ³a tá»‘t hÆ¡n khi há»c dáº§n cÃ¡c nhiá»‡m vá»¥ má»›i.

5.	SiÃªu há»c táº­p (Meta-Learning)

SiÃªu há»c táº­p lÃ  má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c cÃ³ thá»ƒ giÃºp cÃ¡c mÃ´ hÃ¬nh thÃ­ch á»©ng nhanh chÃ³ng vá»›i cÃ¡c nhiá»‡m vá»¥ má»›i. CÃ¡c thuáº­t toÃ¡n siÃªu há»c táº­p Ä‘Ã o táº¡o cÃ¡c mÃ´ hÃ¬nh cÃ¡ch há»c, giÃºp chÃºng tiáº¿p thu kiáº¿n thá»©c má»›i hiá»‡u quáº£ hÆ¡n.

###### 3.1.2.2 Sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡ (Evaluation Metrics)

Viá»‡c phÃ¡t triá»ƒn cÃ¡c sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡ phÃ¹ há»£p cho viá»‡c há»c táº­p liÃªn tá»¥c lÃ  má»™t thÃ¡ch thá»©c vÃ¬ cÃ¡c sá»‘ liá»‡u truyá»n thá»‘ng cÃ³ thá»ƒ khÃ´ng náº¯m báº¯t Ä‘áº§y Ä‘á»§ kháº£ nÄƒng ghi nhá»› cÃ¡c nhiá»‡m vá»¥ cÅ© cá»§a mÃ´ hÃ¬nh trong khi há»c cÃ¡c nhiá»‡m vá»¥ má»›i. CÃ¡c sá»‘ liá»‡u nhÆ° Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c tÃ¡c vá»¥ (MAOT) hoáº·c hiá»‡u suáº¥t phÃ¡t láº¡i bá»™ nhá»› thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng.

#### 3.1.3 CÃ¡c bÆ°á»›c vÃ  chiáº¿n lÆ°á»£c Ä‘á»ƒ thá»±c hiá»‡n Continual Learning

1.	Quáº£n lÃ½ dá»¯ liá»‡u:

â€¢	Thiáº¿t láº­p há»‡ thá»‘ng quáº£n lÃ½ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½ cÃ¡c luá»“ng dá»¯ liá»‡u hoáº·c tÃ¡c vá»¥ Ä‘áº¿n.

â€¢	LÆ°u trá»¯ dá»¯ liá»‡u trong quÃ¡ khá»© vÃ  lÃ m cho nÃ³ cÃ³ thá»ƒ truy cáº­p Ä‘Æ°á»£c Ä‘á»ƒ cáº­p nháº­t mÃ´ hÃ¬nh.

2.	Ká»¹ thuáº­t chÃ­nh quy hÃ³a:

â€¢	Sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t chÃ­nh quy hÃ³a Ä‘á»ƒ báº£o vá»‡ cÃ¡c tham sá»‘ mÃ´ hÃ¬nh quan trá»ng liÃªn quan Ä‘áº¿n cÃ¡c tÃ¡c vá»¥ trÆ°á»›c Ä‘Ã³.

â€¢	CÃ¡c vÃ­ dá»¥ bao gá»“m Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI) vÃ  chÃ­nh quy hÃ³a dá»±a trÃªn Ä‘Æ°á»ng dáº«n.

4.	Há»c trá»±c tuyáº¿n:

â€¢	Triá»ƒn khai há»c táº­p trá»±c tuyáº¿n, trong Ä‘Ã³ mÃ´ hÃ¬nh cáº­p nháº­t liÃªn tá»¥c khi cÃ³ dá»¯ liá»‡u má»›i.

â€¢	Sá»­ dá»¥ng cÃ¡c cáº­p nháº­t nhá» hoáº·c cáº­p nháº­t gia tÄƒng Ä‘á»ƒ thÃ­ch á»©ng vá»›i thÃ´ng tin má»›i.

5.	PhÃ¡t láº¡i bá»™ nhá»›

â€¢	Triá»ƒn khai há»c táº­p trá»±c tuyáº¿n, trong Ä‘Ã³ mÃ´ hÃ¬nh cáº­p nháº­t liÃªn tá»¥c khi cÃ³ dá»¯ liá»‡u má»›i.

â€¢	Sá»­ dá»¥ng cÃ¡c cáº­p nháº­t nhá» hoáº·c cáº­p nháº­t gia tÄƒng Ä‘á»ƒ thÃ­ch á»©ng vá»›i thÃ´ng tin má»›i.

6.	Chuyá»ƒn tiáº¿p há»c táº­p

â€¢	Triá»ƒn khai há»c táº­p trá»±c tuyáº¿n, trong Ä‘Ã³ mÃ´ hÃ¬nh cáº­p nháº­t liÃªn tá»¥c khi cÃ³ dá»¯ liá»‡u má»›i.

â€¢	Sá»­ dá»¥ng cÃ¡c cáº­p nháº­t nhá» hoáº·c cáº­p nháº­t gia tÄƒng Ä‘á»ƒ thÃ­ch á»©ng vá»›i thÃ´ng tin má»›i.

7.	Sá»­a Ä‘á»•i kiáº¿n trÃºc

â€¢	Thá»­ nghiá»‡m cÃ¡c sá»­a Ä‘á»•i kiáº¿n trÃºc cho phÃ©p mÃ´ hÃ¬nh thÃ­ch á»©ng vÃ  má»Ÿ rá»™ng khi cÃ³ nhiá»‡m vá»¥ má»›i.

â€¢	Progressive neural networks, kiáº¿n trÃºc mÃ´-Ä‘un vÃ  cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ má»Ÿ rá»™ng lÃ  nhá»¯ng vÃ­ dá»¥.

8.	ÄÃ¡nh giÃ¡ thÆ°á»ng xuyÃªn

â€¢	LiÃªn tá»¥c Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ hoáº¡t Ä‘á»™ng cá»§a mÃ´ hÃ¬nh trÃªn cáº£ nhiá»‡m vá»¥ má»›i vÃ  cÅ©.

â€¢	Sá»­ dá»¥ng cÃ¡c sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡ thÃ­ch há»£p, cháº³ng háº¡n nhÆ° Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ (MAOT), Ä‘á»ƒ theo dÃµi tiáº¿n Ä‘á»™.

9.	Tá»· lá»‡ quÃªn Ä‘á»™ng

â€¢	Triá»ƒn khai cÃ¡c chiáº¿n lÆ°á»£c hoáº·c tá»· lá»‡ quÃªn linh hoáº¡t cho mÃ´ hÃ¬nh Ä‘á»ƒ kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ quÃªn thÃ´ng tin cÅ©.

â€¢	LÃ m cho quÃ¡ trÃ¬nh quÃªn thÃ­ch á»©ng vá»›i táº§m quan trá»ng cá»§a cÃ¡c nhiá»‡m vá»¥ trong quÃ¡ khá»©.

10.	SiÃªu há»c táº­p

â€¢	KhÃ¡m phÃ¡ cÃ¡c ká»¹ thuáº­t siÃªu há»c táº­p, trong Ä‘Ã³ mÃ´ hÃ¬nh há»c cÃ¡ch thÃ­ch á»©ng nhanh chÃ³ng vá»›i cÃ¡c nhiá»‡m vá»¥ má»›i báº±ng cÃ¡ch Ä‘Ã o táº¡o vá» nhiá»u nhiá»‡m vá»¥ khÃ¡c nhau.

11.	CÆ¡ cháº¿ phÃ¡t hiá»‡n trÃ´i dáº¡t

â€¢	PhÃ¡t triá»ƒn cÃ¡c cÆ¡ cháº¿ Ä‘á»ƒ phÃ¡t hiá»‡n sá»± trÃ´i dáº¡t khÃ¡i niá»‡m hoáº·c nhá»¯ng thay Ä‘á»•i trong phÃ¢n phá»‘i dá»¯ liá»‡u.

â€¢	Cáº­p nháº­t mÃ´ hÃ¬nh kÃ­ch hoáº¡t khi phÃ¡t hiá»‡n sai lá»‡ch khÃ¡i niá»‡m quan trá»ng.

12.	NhÃ£n nhiá»‡m vá»¥

â€¢	Sá»­ dá»¥ng nhÃ£n nhiá»‡m vá»¥ hoáº·c siÃªu thÃ´ng tin Ä‘á»ƒ hÆ°á»›ng dáº«n quÃ¡ trÃ¬nh há»c táº­p cá»§a mÃ´ hÃ¬nh náº¿u cÃ³.

â€¢	ThÃ´ng tin vá» nhiá»‡m vá»¥ cá»¥ thá»ƒ cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh giá»¯ láº¡i hoáº·c quÃªn thÃ´ng tin má»™t cÃ¡ch cÃ³ chá»n lá»c.

13.	Báº£o trÃ¬ thÆ°á»ng xuyÃªn

â€¢	CÃ¡c mÃ´ hÃ¬nh há»c táº­p liÃªn tá»¥c Ä‘Ã²i há»i pháº£i duy trÃ¬ vÃ  giÃ¡m sÃ¡t thÆ°á»ng xuyÃªn.

â€¢	Cáº­p nháº­t vÃ  tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh khi cÃ³ dá»¯ liá»‡u má»›i hoáº·c khi mÃ´i trÆ°á»ng thay Ä‘á»•i.

14.	CÃ¢n báº±ng dá»¯ liá»‡u

â€¢	Giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» máº¥t cÃ¢n báº±ng dá»¯ liá»‡u cÃ³ thá»ƒ phÃ¡t sinh khi cÃ³ nhiá»‡m vá»¥ hoáº·c luá»“ng dá»¯ liá»‡u má»›i.

â€¢	Äáº£m báº£o ráº±ng mÃ´ hÃ¬nh khÃ´ng overfiting vá»›i dá»¯ liá»‡u gáº§n Ä‘Ã¢y nháº¥t.

15.	Bá»™ dá»¯ liá»‡u vÃ  nhiá»…m vá»¥ Benchmark

â€¢	ÄÃ¡nh giÃ¡ cÃ¡c thuáº­t toÃ¡n há»c táº­p liÃªn tá»¥c cá»§a báº¡n trÃªn cÃ¡c táº­p dá»¯ liá»‡u vÃ  nhiá»‡m vá»¥ tiÃªu chuáº©n Ä‘á»ƒ so sÃ¡nh hiá»‡u suáº¥t cá»§a chÃºng vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³.

#### 3.1.4 4 thuáº­t toÃ¡n Continual Learning

##### 3.1.4.1 Progressive Neural Networks (PNNs)

Máº¡ng tháº§n kinh tiáº¿n bá»™ (PNN) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ há»c dáº§n dáº§n cÃ¡c nhiá»‡m vá»¥ má»›i trong khi váº«n duy trÃ¬ kiáº¿n thá»©c vá» cÃ¡c nhiá»‡m vá»¥ Ä‘Ã£ biáº¿t trÆ°á»›c Ä‘Ã³. Ã tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau PNN lÃ  má»Ÿ rá»™ng cÃ´ng suáº¥t cá»§a mÃ´ hÃ¬nh khi cÃ³ nhiá»‡m vá»¥ má»›i. Thay vÃ¬ sá»­ dá»¥ng má»™t máº¡ng nÆ¡ron Ä‘Æ¡n láº», PNN sá»­ dá»¥ng má»™t táº­p há»£p máº¡ng. Má»—i máº¡ng trong nhÃ³m Ä‘Æ°á»£c dÃ nh riÃªng cho má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ. Má»™t máº¡ng lÆ°á»›i tháº§n kinh má»›i Ä‘Æ°á»£c thÃªm vÃ o táº­p há»£p khi má»™t nhiá»‡m vá»¥ má»›i Ä‘Æ°á»£c Ä‘Æ°a ra. Sau Ä‘Ã³, mÃ´ hÃ¬nh káº¿t há»£p Ä‘áº§u ra cá»§a táº¥t cáº£ cÃ¡c máº¡ng Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n.

Lá»£i Ã­ch cá»§a PNN lÃ  chÃºng ngÄƒn cháº·n sá»± quÃªn lÃ£ng nghiÃªm trá»ng báº±ng cÃ¡ch cÃ´ láº­p kiáº¿n thá»©c liÃªn quan Ä‘áº¿n tá»«ng nhiá»‡m vá»¥ trong cÃ¡c máº¡ng chuyÃªn dá»¥ng. Tuy nhiÃªn, táº­p há»£p cÃ³ thá»ƒ trá»Ÿ nÃªn lá»›n khi cÃ³ nhiá»u nhiá»‡m vá»¥ Ä‘Æ°á»£c há»c, Ä‘iá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n tÄƒng lÃªn.

##### 3.1.4.2 Learning without Forgetting (LwF)

Há»c mÃ  khÃ´ng quÃªn (LwF) lÃ  má»™t phÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n táº­n dá»¥ng viá»‡c cháº¯t lá»c kiáº¿n thá»©c Ä‘á»ƒ giáº£i quyáº¿t tÃ¬nh tráº¡ng quÃªn lÃ£ng nghiÃªm trá»ng. Ã tÆ°á»Ÿng lÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c lÃ m máº¡ng giÃ¡o viÃªn vÃ  máº¡ng lÆ°á»›i tháº§n kinh má»›i lÃ m há»c sinh. Khi há»c má»™t nhiá»‡m vá»¥ má»›i, máº¡ng há»c sinh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ báº¯t chÆ°á»›c dá»± Ä‘oÃ¡n cá»§a giÃ¡o viÃªn vá» dá»¯ liá»‡u cÅ© vÃ  má»›i. QuÃ¡ trÃ¬nh nÃ y giÃºp máº¡ng há»c sinh ghi nhá»› Ä‘Æ°á»£c kiáº¿n thá»©c tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³.

LwF cÃ³ hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n vÃ¬ nÃ³ khÃ´ng yÃªu cáº§u duy trÃ¬ má»™t táº­p há»£p máº¡ng lá»›n. NÃ³ Ä‘áº·c biá»‡t thÃ nh cÃ´ng trong cÃ¡c tÃ¬nh huá»‘ng mÃ  viá»‡c tinh chá»‰nh mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c lÃ  cÃ³ lá»£i.

##### 3.1.4.3 iCaRL (Incremental Classifier and Representation Learning)

iCaRL (Há»c phÃ¢n loáº¡i vÃ  biá»ƒu diá»…n tÄƒng dáº§n) lÃ  má»™t thuáº­t toÃ¡n Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c nhiá»‡m vá»¥ há»c táº­p liÃªn tá»¥c liÃªn quan Ä‘áº¿n phÃ¢n loáº¡i. NÃ³ káº¿t há»£p cÃ¡c chiáº¿n lÆ°á»£c Ä‘á»ƒ há»c biá»ƒu diá»…n tÃ­nh nÄƒng vÃ  lÆ°u trá»¯ máº«u dÃ nh riÃªng cho lá»›p. MÃ´ hÃ¬nh duy trÃ¬ má»™t táº­p há»£p cÃ¡c máº«u (máº«u Ä‘áº¡i diá»‡n) tá»« má»—i lá»›p Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³. Khi cÃ¡c lá»›p má»›i Ä‘Æ°á»£c giá»›i thiá»‡u, iCaRL sá»­ dá»¥ng cÃ¡c máº«u nÃ y Ä‘á»ƒ lÆ°u giá»¯ kiáº¿n thá»©c vá» cÃ¡c lá»›p cÅ©.

iCaRL ráº¥t phÃ¹ há»£p cho cÃ¡c nhiá»‡m vá»¥ cáº§n quan tÃ¢m Ä‘áº¿n sá»± máº¥t cÃ¢n báº±ng lá»›p vÃ¬ nÃ³ Ä‘áº£m báº£o ráº±ng mÃ´ hÃ¬nh giá»¯ láº¡i kiáº¿n thá»©c cá»§a cáº£ lá»›p cÅ© vÃ  lá»›p má»›i trong khi thÃ­ch á»©ng vá»›i dá»¯ liá»‡u má»›i.

##### 3.1.4.4 Meta-Learning Approaches

SiÃªu há»c táº­p (Meta-Learning) bao gá»“m cÃ¡c mÃ´ hÃ¬nh Ä‘Ã o táº¡o Ä‘á»ƒ há»c hiá»‡u quáº£ vÃ  cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng cho viá»‡c há»c táº­p liÃªn tá»¥c. Trong siÃªu há»c táº­p Ä‘á»ƒ há»c liÃªn tá»¥c, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o vá» nhiá»u nhiá»‡m vá»¥ khÃ¡c nhau Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c chiáº¿n lÆ°á»£c khá»Ÿi táº¡o hoáº·c há»c táº­p tá»‘t nháº±m thÃ­ch á»©ng nhanh chÃ³ng vá»›i cÃ¡c nhiá»‡m vá»¥ má»›i.

CÃ¡c ká»¹ thuáº­t siÃªu há»c Ä‘Ã£ cho tháº¥y nhiá»u há»©a háº¹n trong viá»‡c giáº£m thiá»ƒu tÃ¬nh tráº¡ng quÃªn lÃ£ng nghiÃªm trá»ng báº±ng cÃ¡ch trang bá»‹ cho cÃ¡c mÃ´ hÃ¬nh má»™t Ä‘iá»ƒm khá»Ÿi Ä‘áº§u vá»¯ng cháº¯c Ä‘á»ƒ há»c cÃ¡c nhiá»‡m vá»¥ má»›i.
##### 3.1.5 Thuá»‘c Ä‘o Ä‘Ã¡nh giÃ¡ vá» Continual Learning

1.	Äá»™ chÃ­nh xÃ¡c trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ (MAOT)
2.	Duy trÃ¬ hiá»‡u suáº¥t nhiá»‡m vá»¥
3.	Hiá»‡u suáº¥t phÃ¡t láº¡i bá»™ nhá»›
4.	Sá»‘ liá»‡u dÃ nh riÃªng cho nhiá»‡m vá»¥
5.	Tá»‘c Ä‘á»™ thÃ­ch á»©ng vÃ  sá»­ dá»¥ng tÃ i nguyÃªn
6.	Quy trÃ¬nh Ä‘Ã¡nh giÃ¡

### 3.2 Test Production

<img src="picture/3.3.png">

HÃ¬nh 3.3 SÆ¡ Ä‘á»“ há»‡ thá»‘ng ML vÃ  mÃ´ hÃ¬nh ML

â€¢	á» giá»¯a lÃ  mÃ´ hÃ¬nh ML - má»™t táº¡o pháº©m Ä‘Æ°á»£c táº¡o bá»Ÿi quÃ¡ trÃ¬nh Ä‘Ã o táº¡o, mÃ´ hÃ¬nh nÃ y nháº­n Ä‘áº§u vÃ o vÃ  táº¡o ra Ä‘áº§u ra.

â€¢	Há»‡ thá»‘ng Ä‘Ã o táº¡o (Training system) láº¥y mÃ£ vÃ  dá»¯ liá»‡u lÃ m Ä‘áº§u vÃ o vÃ  táº¡o ra mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o (ML model) lÃ m Ä‘áº§u ra.

â€¢	Há»‡ thá»‘ng dá»± Ä‘oÃ¡n (Prediction system) tiáº¿p nháº­n vÃ  xá»­ lÃ½ trÆ°á»›c dá»¯ liá»‡u thÃ´, táº£i mÃ´ hÃ¬nh ML Ä‘Ã£ Ä‘Ã o táº¡o, táº£i trá»ng sá»‘ mÃ´ hÃ¬nh, gá»i model.predict() trÃªn dá»¯ liá»‡u, xá»­ lÃ½ háº­u ká»³ cÃ¡c káº¿t quáº£ Ä‘áº§u ra vÃ  tráº£ vá» dá»± Ä‘oÃ¡n (Predictions).

â€¢	Sau khi báº¡n triá»ƒn khai há»‡ thá»‘ng dá»± Ä‘oÃ¡n cá»§a mÃ¬nh lÃªn mÃ´i trÆ°á»ng trá»±c tuyáº¿n, há»‡ thá»‘ng cung cáº¥p sáº½ tiáº¿p nháº­n yÃªu cáº§u tá»« ngÆ°á»i dÃ¹ng, tÄƒng giáº£m quy mÃ´ Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u lÆ°u lÆ°á»£ng truy cáº­p vÃ  Ä‘Æ°a ra dá»± Ä‘oÃ¡n ngÆ°á»£c láº¡i cho nhá»¯ng ngÆ°á»i dÃ¹ng Ä‘Ã³.

â€¢	ToÃ n bá»™ há»‡ thá»‘ng ML Ä‘Ã³ng vÃ²ng láº·p báº±ng cÃ¡ch thu tháº­p dá»¯ liá»‡u sáº£n xuáº¥t (cáº£ dá»± Ä‘oÃ¡n mÃ  mÃ´ hÃ¬nh táº¡o ra vÃ  pháº£n há»“i bá»• sung tá»« ngÆ°á»i dÃ¹ng, sá»‘ liá»‡u kinh doanh hoáº·c ngÆ°á»i gáº¯n nhÃ£n) vÃ  gá»­i chÃºng trá»Ÿ láº¡i mÃ´i trÆ°á»ng ngoáº¡i tuyáº¿n.

â€¢	Há»‡ thá»‘ng ghi nhÃ£n (Labeling system) láº¥y dá»¯ liá»‡u thÃ´ nhÃ¬n tháº¥y trong quÃ¡ trÃ¬nh sáº£n xuáº¥t, giÃºp báº¡n nháº­n thÃ´ng tin Ä‘áº§u vÃ o tá»« ngÆ°á»i gáº¯n nhÃ£n vÃ  cung cáº¥p nhÃ£n cho dá»¯ liá»‡u Ä‘Ã³.

â€¢	Há»‡ thá»‘ng lÆ°u trá»¯ vÃ  xá»­ lÃ½ trÆ°á»›c (Storage and preprocessing system) lÆ°u trá»¯ vÃ  xá»­ lÃ½ trÆ°á»›c dá»¯ liá»‡u Ä‘Æ°á»£c dÃ¡n nhÃ£n trÆ°á»›c khi chuyá»ƒn nÃ³ trá»Ÿ láº¡i há»‡ thá»‘ng Ä‘Ã o táº¡o (Training system).
Kiá»ƒm tra há»‡ thá»‘ng ML Ä‘Ãºng cÃ¡ch lÃ  cÃ¡c bÃ i kiá»ƒm tra cÃ³ thá»ƒ cháº¡y cho tá»«ng thÃ nh pháº§n há»‡ thá»‘ng vÃ  xuyÃªn qua border cá»§a cÃ¡c component nÃ y.

#### 3.2.1 Kiá»ƒm tra cÆ¡ sá»Ÿ háº¡ táº§ng

<img src="picture/3.4.png">

HÃ¬nh 3.4 Kiá»ƒm tra cÆ¡ sá»Ÿ háº¡ táº§ng

Kiá»ƒm tra cÆ¡ sá»Ÿ háº¡ táº§ng lÃ  cÃ¡c bÃ i kiá»ƒm tra Ä‘Æ¡n vá»‹ cho há»‡ thá»‘ng Ä‘Ã o táº¡o. ChÃºng giÃºp trÃ¡nh Ä‘Æ°á»£c lá»—i trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o vÃ  cÃ³ thá»ƒ kiá»ƒm tra Ä‘Æ¡n vá»‹ mÃ£ Ä‘Ã o táº¡o giá»‘ng nhÆ° báº¥t ká»³ mÃ£ nÃ o khÃ¡c. Má»™t phÆ°Æ¡ng phÃ¡p phá»• biáº¿n khÃ¡c lÃ  thÃªm cÃ¡c bÃ i kiá»ƒm tra single-batch hoáº·c single-epoch Ä‘á»ƒ kiá»ƒm tra hiá»‡u suáº¥t sau khi cháº¡y chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o rÃºt gá»n trÃªn má»™t táº­p dá»¯ liá»‡u nhá», giÃºp phÃ¡t hiá»‡n cÃ¡c há»“i quy rÃµ rÃ ng Ä‘á»‘i vá»›i mÃ£ Ä‘Ã o táº¡o.

#### 3.2.2 Kiá»ƒm tra Ä‘Ã o táº¡o
 
<img src="picture/3.5.png">

HÃ¬nh 3.5 Kiá»ƒm tra Ä‘Ã o táº¡o

BÃ i kiá»ƒm tra Ä‘Ã o táº¡o lÃ  bÃ i kiá»ƒm tra tÃ­ch há»£p giá»¯a há»‡ thá»‘ng dá»¯ liá»‡u vÃ  há»‡ thá»‘ng Ä‘Ã o táº¡o. Äá»ƒ Ä‘áº£m báº£o ráº±ng cÃ´ng viá»‡c Ä‘Ã o táº¡o cÃ³ thá»ƒ tÃ¡i táº¡o Ä‘Æ°á»£c.

CÃ³ thá»ƒ láº¥y má»™t táº­p dá»¯ liá»‡u cá»‘ Ä‘á»‹nh vÃ  cháº¡y má»™t chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o Ä‘áº§y Ä‘á»§ hoáº·c rÃºt gá»n trÃªn Ä‘Ã³. Sau Ä‘Ã³, kiá»ƒm tra vÃ  Ä‘áº£m báº£o ráº±ng hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh trÃªn mÃ´ hÃ¬nh má»›i Ä‘Æ°á»£c Ä‘Ã o táº¡o váº«n nháº¥t quÃ¡n vá»›i hiá»‡u suáº¥t tham chiáº¿u.

Má»™t tÃ¹y chá»n khÃ¡c lÃ  kÃ©o má»™t cá»­a sá»• dá»¯ liá»‡u trÆ°á»£t (cÃ³ thá»ƒ lÃ  má»™t cá»­a sá»• má»›i cho vÃ i ngÃ y má»™t láº§n) vÃ  cháº¡y cÃ¡c bÃ i kiá»ƒm tra huáº¥n luyá»‡n trÃªn cá»­a sá»• Ä‘Ã³.

#### 3.2.3 Kiá»ƒm tra chá»©c nÄƒng
 
<img src="picture/3.6.png">

HÃ¬nh 3.6 Kiá»ƒm tra chá»©c nÄƒng

Kiá»ƒm tra chá»©c nÄƒng lÃ  kiá»ƒm tra Ä‘Æ¡n vá»‹ cho há»‡ thá»‘ng dá»± Ä‘oÃ¡n. ChÃºng giÃºp trÃ¡nh hiá»‡n tÆ°á»£ng há»“i quy trong mÃ£ táº¡o nÃªn cÆ¡ sá»Ÿ háº¡ táº§ng dá»± Ä‘oÃ¡n.

â€¢	CÃ³ thá»ƒ kiá»ƒm tra Ä‘Æ¡n vá»‹ mÃ£ dá»± Ä‘oÃ¡n giá»‘ng nhÆ° báº¥t ká»³ mÃ£ nÃ o khÃ¡c.

â€¢	Cá»¥ thá»ƒ Ä‘á»‘i vá»›i há»‡ thá»‘ng ML, cÃ³ thá»ƒ táº£i mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c vÃ  kiá»ƒm tra dá»± Ä‘oÃ¡n cá»§a nÃ³ trÃªn má»™t sá»‘ vÃ­ dá»¥ chÃ­nh.

#### 3.2.4 Kiá»ƒm tra Ä‘Ã¡nh giÃ¡
 
<img src="picture/3.7.png">

HÃ¬nh 3.7 Kiá»ƒm tra Ä‘Ã¡nh giÃ¡

Kiá»ƒm tra Ä‘Ã¡nh giÃ¡ lÃ  kiá»ƒm tra tÃ­ch há»£p giá»¯a há»‡ thá»‘ng Ä‘Ã o táº¡o vÃ  há»‡ thá»‘ng dá»± Ä‘oÃ¡n. Äáº£m báº£o ráº±ng má»™t mÃ´ hÃ¬nh má»›i Ä‘Æ°á»£c Ä‘Ã o táº¡o Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ Ä‘Æ°a vÃ o sáº£n xuáº¥t. Nhá»¯ng Ä‘iá»u nÃ y táº¡o nÃªn pháº§n lá»›n nhá»¯ng Ä‘iá»ƒm Ä‘á»™c Ä‘Ã¡o khi thá»­ nghiá»‡m há»‡ thá»‘ng ML.

#### 3.2.5 Kiá»ƒm tra Shadow
 

<img src="picture/3.8.png">

HÃ¬nh 3.8 Kiá»ƒm tra Shadow

Kiá»ƒm tra shadow lÃ  kiá»ƒm tra tÃ­ch há»£p giá»¯a há»‡ thá»‘ng dá»± Ä‘oÃ¡n vÃ  há»‡ thá»‘ng phÃ¢n phá»‘. ChÃºng giÃºp phÃ¡t hiá»‡n lá»—i sáº£n xuáº¥t trÆ°á»›c khi lá»—i Ä‘Ã³ gáº·p ngÆ°á»i dÃ¹ng. Trong nhiá»u cÃ i Ä‘áº·t, cÃ¡c mÃ´ hÃ¬nh (Ä‘Æ°á»£c xÃ¢y dá»±ng trong cÃ¡c khung nhÆ° sklearn, Pytorch, TensorFlow, â€¦) Ä‘Æ°á»£c phÃ¡t triá»ƒn tÃ¡ch biá»‡t vá»›i há»‡ thá»‘ng pháº§n má»m hiá»‡n cÃ³. VÃ­ dá»¥: mÃ´ hÃ¬nh gáº¯n cá» cÃ¡c tweet khÃ´ng phÃ¹ há»£p cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¡t triá»ƒn trong TensorFlow trÃªn má»™t táº­p dá»¯ liá»‡u tÄ©nh chá»© khÃ´ng pháº£i trá»±c tiáº¿p trong mÃ´i trÆ°á»ng phÃ¡t trá»±c tuyáº¿n cá»§a kiáº¿n trÃºc pháº§n má»m rá»™ng hÆ¡n. Do há»‡ thá»‘ng dá»± Ä‘oÃ¡n vÃ  há»‡ thá»‘ng cung cáº¥p Ä‘Æ°á»£c phÃ¡t triá»ƒn á»Ÿ cÃ¡c cÃ i Ä‘áº·t khÃ¡c nhau vá»›i cÃ¡c giáº£ Ä‘á»‹nh vÃ  mÃ´i trÆ°á»ng khÃ¡c nhau nÃªn cÃ³ nhiá»u cÆ¡ há»™i Ä‘á»ƒ lá»—i xÃ¢m nháº­p. Nhá»¯ng lá»—i nÃ y cÃ³ thá»ƒ khÃ³ phÃ¡t hiá»‡n trÆ°á»›c khi tÃ­ch há»£p, vÃ¬ váº­y, thá»­ nghiá»‡m shadow cÃ³ thá»ƒ giÃºp xÃ¡c Ä‘á»‹nh chÃºng trÆ°á»›c.

#### 3.2.6 Thá»­ nghiá»‡m A/B
 
<img src="picture/3.9.png">

HÃ¬nh 3.9 Thá»­ nghiá»‡m A/B

Kiá»ƒm tra shadow Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh nhÆ° má»™t pháº§n cá»§a kiáº¿n trÃºc pháº§n má»m rá»™ng hÆ¡n nhÆ°ng khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n ngÆ°á»i dÃ¹ng. Thá»­ nghiá»‡m A/B Ä‘áº£m nháº­n vai trÃ² nÃ y. Thá»­ nghiá»‡m A/B lÃ  má»™t phÆ°Æ¡ng phÃ¡p phá»• biáº¿n trong cÃ´ng nghá»‡ pháº§n má»m, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c há»‡ thá»‘ng web. Thá»­ nghiá»‡m A/B Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  â€œmá»™t quÃ¡ trÃ¬nh thá»­ nghiá»‡m ngáº«u nhiÃªn trong Ä‘Ã³ hai hoáº·c nhiá»u phiÃªn báº£n cá»§a má»™t biáº¿n (trang web, thÃ nh pháº§n trang, ...) Ä‘Æ°á»£c hiá»ƒn thá»‹ cho cÃ¡c phÃ¢n khÃºc khÃ¡ch truy cáº­p trang web khÃ¡c nhau cÃ¹ng lÃºc Ä‘á»ƒ xÃ¡c Ä‘á»‹nh phiÃªn báº£n nÃ o Ä‘á»ƒ láº¡i tÃ¡c Ä‘á»™ng tá»‘i Ä‘a vÃ  thÃºc Ä‘áº©y cÃ¡c chá»‰ sá»‘ kinh doanh.

#### 3.2.7 Kiá»ƒm tra ghi nhÃ£n
 
<img src="picture/3.10.png">

HÃ¬nh 3.10 Kiá»ƒm tra ghi nhÃ£n

CÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y hoáº¡t Ä‘á»™ng theo mÃ´ hÃ¬nh GIGO: garbage in, garbage out. Äá»ƒ ngÄƒn cÃ¡c nhÃ£n cháº¥t lÆ°á»£ng kÃ©m bá»‹ cáº¯t xÃ©n vÃ  lÃ m há»ng mÃ´ hÃ¬nh, cáº§n kiá»ƒm tra Ä‘Æ¡n vá»‹ cÃ¡c há»‡ thá»‘ng vÃ  quy trÃ¬nh ghi nhÃ£n.

#### 3.2.8 Kiá»ƒm tra ká»³ vá»ng
 
<img src="picture/3.11.png">

HÃ¬nh 3.11 Kiá»ƒm tra ká»³ vá»ng

Kiá»ƒm tra ká»³ vá»ng giáº£i quyáº¿t há»‡ thá»‘ng lÆ°u trá»¯ vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u. Vá» cÆ¡ báº£n, chÃºng lÃ  cÃ¡c bÃ i kiá»ƒm tra Ä‘Æ¡n vá»‹ cho dá»¯ liá»‡u. ChÃºng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c váº¥n Ä‘á» vá» cháº¥t lÆ°á»£ng dá»¯ liá»‡u vÃ  dá»¯ liá»‡u xáº¥u trÆ°á»›c khi chÃºng Ä‘Æ°á»£c Ä‘Æ°a vÃ o há»‡ thá»‘ng.





## TÃ€I LIá»†U THAM KHáº¢O
### Tiáº¿ng Viá»‡t
1.	Machine Learning cÆ¡ báº£n, BÃ i 7: Gradient Descent (pháº§n 1/2), machinelearningcoban.com
https://machinelearningcoban.com/2017/01/12/gradientdescent/ 
2.	Machine Learning cÆ¡ báº£n, BÃ i 8: Gradient Descent (pháº§n 2/2), machinelearningcoban.com
https://machinelearningcoban.com/2017/01/16/gradientdescent2/ 
3.	VÄ©nh Anh NghiÃªm QuÃ¢n â€“ Nguyá»…n LÃª Trung ThÃ nh â€“ Nguyá»…n Thá»‹ Lan Anh, ÄÃNH GIÃ HIá»†U NÄ‚NG Cá»¦A CÃC THUáº¬T TOÃN Tá»I Æ¯U TRONG MÃ” HÃŒNH Há»ŒC SÃ‚U Äá»I Vá»šI BÃ€I TOÃN PHÃ‚N Lá»šP HÃŒNH áº¢NH, Khoa Tin há»c â€“ TrÆ°á»ng ÄHSP Huáº¿.
https://csdlkhoahoc.hueuni.edu.vn/data/2021/5/BaiDangHoiThao.pdf 
4.	VÆ°Æ¡ng Quang PhÆ°á»›c, Nguyá»…n Äá»©c Nháº­t Quang, ÄÃNH GIÃ CÃC THUáº¬T TOÃN Tá»I Æ¯U Äá»I Vá»šI MÃ” HÃŒNH Máº NG NÆ -RON TÃCH CHáº¬P TRONG TÃC Vá»¤ NHáº¬N DIá»†N HÃŒNH áº¢NH, Khoa Äiá»‡n, Äiá»‡n tá»­ vÃ  CÃ´ng nghá»‡ váº­t liá»‡u, TrÆ°á»ng Äáº¡i há»c Khoa há»c, Äáº¡i há»c Huáº¿.
https://jos.husc.edu.vn/backup/upload/vol_18/no_1/668_fulltext_4.%C4%90TVT%20-%20Phuoc%20-%20Vuong%20Quang%20Phuoc.pdf 
5.	Tráº§n Trung Trá»±c, Optimizer- Hiá»ƒu sÃ¢u vá» cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u ( GD,SGD,Adam,..), viblo.asia.
https://viblo.asia/p/optimizer-hieu-sau-ve-cac-thuat-toan-toi-uu-gdsgdadam-Qbq5QQ9E5D8 


### Tiáº¿ng Anh
6.	Zeiler, M. D. (2012), Adadelta: an adaptive learning rate method, arXiv preprint arXiv:1212.5701.
7.	Reddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237.
8.	Continual Learning, paperswithcode.com.
https://paperswithcode.com/task/continual-learning 
9.	Z. Chen and B. Liu, 2018, Lifelong Machine Learning, C H A P T E R 4 Continual Learning and Catastrophic Forgetting.
https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf 
10.	Neri Van Otten (Oct 3, 2023), Continual Learning Made Simple, How To Get Started & Top 4 Models, https://spotintelligence.com.
https://spotintelligence.com/2023/10/03/continual-learning/#Top_4_continual_learning_algorithms 





