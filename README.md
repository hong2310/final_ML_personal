# final_ML_personal
Nguyá»…n Thá»‹ Thu Há»“ng - 52100962

# Dá»° ÃN CUá»I Ká»²
# NHáº¬P MÃ”N Há»ŒC MÃY
BÃ€I 1: TrÃ¬nh bÃ y má»™t bÃ i nghiÃªn cá»©u, Ä‘Ã¡nh giÃ¡ cá»§a em vá» cÃ¡c váº¥n Ä‘á» sau:
1.	TÃ¬m hiá»ƒu, so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p Optimizer trong huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y;
2.	TÃ¬m hiá»ƒu vá» Continual Learning vÃ  Test Production khi xÃ¢y dá»±ng má»™t giáº£i phÃ¡p há»c mÃ¡y Ä‘á»ƒ giáº£i quyáº¿t má»™t bÃ i toÃ¡n nÃ o Ä‘Ã³;

## CHÆ¯Æ NG 1 â€“ Tá»”NG QUAN Vá»€ Há»ŒC MÃY
### 1.1 Há»c mÃ¡y lÃ  gÃ¬?
Machine learning (há»c mÃ¡y hay mÃ¡y há»c) lÃ  má»™t nhÃ¡nh con cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o (AI) vÃ  khoa há»c mÃ¡y tÃ­nh. Machine learning sá»­ dá»¥ng dá»¯ liá»‡u, thuáº­t toÃ¡n Ä‘áº§u vÃ o Ä‘á»ƒ tá»± xá»­ lÃ½ cÃ¡c váº¥n Ä‘á» vÃ  liÃªn tá»¥c tá»‘i Æ°u Ä‘á»ƒ táº¡o ra nhá»¯ng phÆ°Æ¡ng Ã¡n xá»­ lÃ½ má»›i hiá»‡u quáº£ hÆ¡n, phÃ¹ há»£p hÆ¡n, giá»‘ng nhÆ° cÃ¡ch thá»©c tá»± há»c cá»§a nÃ£o bá»™ con ngÆ°á»i.

Má»™t cÃ¡ch tá»•ng quÃ¡t, trong cuá»‘n sÃ¡ch Machine Learning cá»§a tÃ¡c giáº£ Tom Mitchell xuáº¥t báº£n nÄƒm 1997, há»c mÃ¡y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau: â€œA computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over timeâ€ (Má»™t chÆ°Æ¡ng trÃ¬nh mÃ¡y tÃ­nh Ä‘Æ°á»£c cho lÃ  há»c Ä‘á»ƒ thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ T tá»« kinh nghiá»‡m E, náº¿u hiá»‡u suáº¥t thá»±c hiá»‡n cÃ´ng viá»‡c T cá»§a nÃ³ Ä‘Æ°á»£c Ä‘o bá»Ÿi chá»‰ sá»‘ hiá»‡u suáº¥t P vÃ  Ä‘Æ°á»£c cáº£i thiá»‡n bá»Ÿi kinh nghiá»‡m E theo thá»i gian).
KhÃ¡c biá»‡t giá»¯a chÆ°Æ¡ng trÃ¬nh láº­p trÃ¬nh truyá»n thá»‘ng vÃ  há»c mÃ¡y.

<img src="picture/1.1.png">

HÃ¬nh 1.1 Minh há»a chÆ°Æ¡ng trÃ¬nh láº­p trÃ¬nh truyá»n thá»‘ng

<img src="picture/1.1.1.png">

HÃ¬nh 1.1 Minh há»a há»c mÃ¡y

Thá»‘ng kÃª vÃ  dá»± Ä‘oÃ¡n lÃ  hai má»¥c Ä‘Ã­ch chÃ­nh cá»§a viá»‡c Ã¡p dá»¥ng machine learning vÃ¬ tháº¿ há»‡ thá»‘ng nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i kháº£ nÄƒng tá»± nghiÃªn cá»©u, cáº£i tiáº¿n báº£n thÃ¢n dá»±a trÃªn nhá»¯ng nguyÃªn lÃ½ Ä‘Æ°á»£c láº­p trÃ¬nh ban Ä‘áº§u. Trong nhiá»u trÆ°á»ng há»£p machine learning sáº½ tá»± Ä‘á» xuáº¥t ra giáº£i phÃ¡p tá»‘i Æ°u mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh trÆ°á»›c. Do Ä‘Ã³, cÃ³ thá»ƒ nÃ³i Machine Learning giá»‘ng nhÆ° má»™t ngÆ°á»i lao Ä‘á»™ng vá»›i kháº£ nÄƒng tá»± há»c, hoÃ n thiá»‡n vÃ  giÃ u kinh nghiá»‡m hÆ¡n theo thá»i gian.

Trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, khi mÃ  kháº£ nÄƒng tÃ­nh toÃ¡n cá»§a cÃ¡c mÃ¡y tÃ­nh Ä‘Æ°á»£c nÃ¢ng lÃªn má»™t táº§m cao má»›i vÃ  lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ Ä‘Æ°á»£c thu tháº­p bá»Ÿi cÃ¡c hÃ£ng cÃ´ng nghá»‡ lá»›n, Machine Learning Ä‘Ã£ tiáº¿n thÃªm má»™t bÆ°á»›c dÃ i vÃ  má»™t lÄ©nh vá»±c má»›i Ä‘Æ°á»£c ra Ä‘á»i gá»i lÃ  Deep Learning (Há»c SÃ¢u). Deep Learning Ä‘Ã£ giÃºp mÃ¡y tÃ­nh thá»±c thi nhá»¯ng viá»‡c tÆ°á»Ÿng chá»«ng nhÆ° khÃ´ng thá»ƒ vÃ o 10 nÄƒm trÆ°á»›c: phÃ¢n loáº¡i cáº£ ngÃ n váº­t thá»ƒ khÃ¡c nhau trong cÃ¡c bá»©c áº£nh, tá»± táº¡o chÃº thÃ­ch cho áº£nh, báº¯t chÆ°á»›c giá»ng nÃ³i vÃ  chá»¯ viáº¿t cá»§a con ngÆ°á»i, giao tiáº¿p vá»›i con ngÆ°á»i, hay tháº­m chÃ­ cáº£ sÃ¡ng tÃ¡c vÄƒn hay Ã¢m nháº¡c, â€¦

<img src="picture/1.2.png">

HÃ¬nh 1.2 Má»‘i quan há»‡ giá»¯a AI, Machine Learning vÃ  Deep Learning

(Nguá»“n: Whatâ€™s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?)

### 1.2 PhÃ¢n loáº¡i Há»c mÃ¡y:

Dá»±a trÃªn cÃ¡c tiÃªu chÃ­ khÃ¡c nhau, ngÆ°á»i ta cÃ³ thá»ƒ phÃ¢n loáº¡i cÃ¡c thuáº­t toÃ¡n Há»c mÃ¡y theo nhiá»u cÃ¡ch khÃ¡c nhau. 

#### 1.2.1 PhÃ¢n loáº¡i theo váº¥n Ä‘á», nhiá»‡m vá»¥ cáº§n giáº£i quyáº¿t:

Dá»±a vÃ o váº¥n Ä‘á», nhiá»‡m vá»¥ cáº§n giáº£i quyáº¿t cá»§a thuáº­t toÃ¡n, ngÆ°á»i ta phÃ¢n loáº¡i cÃ¡c thuáº­t toÃ¡n Há»c mÃ¡y thÃ nh ba loáº¡i:

1.	Há»“i quy (Regression): Giáº£i quyáº¿t bÃ i toÃ¡n dá»± Ä‘oÃ¡n giÃ¡ trá»‹ má»™t Ä‘áº¡i lÆ°á»£ng nÃ o Ä‘Ã³ dá»±a vÃ o giÃ¡ trá»‹ cá»§a cÃ¡c Ä‘áº¡i lÆ°á»£ng liÃªn quan. VÃ­ dá»¥, dá»±a vÃ o cÃ¡c Ä‘áº·c Ä‘iá»ƒm nhÆ° diá»‡n tÃ­ch, sá»‘ phÃ²ng, khoáº£ng cÃ¡ch tá»›i trung tÃ¢mâ€¦Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ cÄƒn nhÃ .
2.	PhÃ¢n lá»›p (Classification): Giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n nháº­n dáº¡ng xem má»™t Ä‘á»‘i tÆ°á»£ng thuá»™c lá»›p nÃ o trong sá»‘ cÃ¡c lá»›p cho trÆ°á»›c. VÃ­ dá»¥, bÃ i toÃ¡n nháº­n diá»‡n chá»¯ viáº¿t, bÃ i toÃ¡n phÃ¢n loáº¡i emailâ€¦thuá»™c cÃ¡c thuáº­t toÃ¡n phÃ¢n lá»›p.
3.	PhÃ¢n cá»¥m (Clustering): Ã tÆ°á»Ÿng cÆ¡ báº£n giá»‘ng vá»›i cÃ¡c thuáº­t toÃ¡n phÃ¢n lá»›p, sá»± khÃ¡c biá»‡t lÃ  á»Ÿ chá»—, trong cÃ¡c bÃ i toÃ¡n phÃ¢n cá»¥m, cÃ¡c cá»¥m chÆ°a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c vÃ  thuáº­t toÃ¡n pháº£i tá»± khÃ¡m phÃ¡ vÃ  phÃ¢n cá»¥m dá»¯ liá»‡u.

<img src="picture/1.3.png">

HÃ¬nh 1.3 CÃ¡c giáº£i thuáº­t Há»c mÃ¡y

(Nguá»“n: https://tailieuhay.vn/tai-lieu/bai-giang-hoc-may-bai-5-cay-phan-loai-va-hoi-quy-nguyen-thanh-tung-7733/ )

#### 1.2.2 PhÃ¢n loáº¡i theo cÃ¡ch mÃ¡y tÃ­nh há»c:

Dá»±a trÃªn cÃ¡ch mÃ¡y tÃ­nh há»c, ngÆ°á»i ta chia cÃ¡c thuáº­t toÃ¡n Há»c mÃ¡y thÃ nh ba loáº¡i:

1.	Há»c táº­p dÆ°á»›i sá»± giÃ¡m sÃ¡t (Supervised learning): Con ngÆ°á»i sáº½ láº­p trÃ¬nh dá»¯ liá»‡u Ä‘áº§u vÃ o bao gá»“m cáº£ cÃ¡ch thá»©c vÃ  phÆ°Æ¡ng Ã¡n mÃ  con ngÆ°á»i mong muá»‘n. PhÆ°Æ¡ng Ã¡n vÃ  Ä‘Ã¡p Ã¡n sáº½ Ä‘Æ°á»£c gáº¯n nhÃ£n, sáº¯p xáº¿p sáºµn vÃ  Machine Learning chá»‰ cáº§n rÃ  soÃ¡t vÃ  tráº£ ra Ä‘Ãºng káº¿t quáº£ cÃ³ trong bá»™ dá»¯ liá»‡u Ä‘Ã£ cÃ³. Tin nháº¯n rÃ¡c Ä‘áº¿n tá»« 1 sá»‘ nguá»“n sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c tÃ¡ch ra khá»i há»™p thÆ° chÃ­nh lÃ  á»©ng dá»¥ng cá»§a machine learning giÃºp phÃ¢n loáº¡i tin nháº¯n trÃªn email.
2.	Há»c táº­p mÃ  khÃ´ng giÃ¡m sÃ¡t (Unsupervised learning): Machine learning chá»‰ Ä‘Æ°á»£c cung cáº¥p cÃ¡c thuáº­t toÃ¡n, cÃ´ng cá»¥ Ä‘á»ƒ tá»± xá»­ lÃ½ mÃ  khÃ´ng biáº¿t trÆ°á»›c káº¿t quáº£. Dá»… tháº¥y nháº¥t viá»‡c á»©ng dá»¥ng cá»§a phÃ¢n loáº¡i nÃ y Ä‘Ã³ lÃ  cÃ¡ nhÃ¢n hÃ³a tráº£i nghiá»‡m khÃ¡ch hÃ ng.Dá»¯ liá»‡u Ä‘áº§u vÃ o bao gá»“m hÃ nh vi, lá»‹ch sá»­ mua mua hÃ ng vÃ  há»‡ thá»‘ng sáº½ dá»± Ä‘oÃ¡n nhá»¯ng sáº£n pháº©m phÃ¹ há»£p vÃ  Ä‘á» xuáº¥t riÃªng cho tá»«ng khÃ¡ch hÃ ng.
3.	Há»c táº­p Ä‘Æ°á»£c giÃ¡m sÃ¡t bÃ¡n pháº§n (Semi-supervised learning): ÄÃ¢y lÃ  phÃ¢n loáº¡i náº±m á»Ÿ giá»¯a cá»§a 2 phÃ¢n loáº¡i trÃªn khi nÃ y dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  1 há»—n há»£p bao gá»“m cáº£ phÆ°Æ¡ng phÃ¡p láº«n Ä‘Ã¡p Ã¡n. Äiá»ƒm khÃ¡c biá»‡t á»Ÿ Ä‘Ã¢y lÃ  phÆ°Æ¡ng Ã¡n vÃ  Ä‘Ã¡p Ã¡n chÆ°a Ä‘Æ°á»£c nhÃ³m láº¡i thÃ nh tá»«ng bá»™. NhÆ° váº­y machine learning pháº£i tá»± tÃ¬m ra cÃ¡ch giáº£i nÃ o tÆ°Æ¡ng thÃ­ch vá»›i Ä‘Ã¡p Ã¡n nÃ o trong bá»™ dá»¯ liá»‡u sáºµn cÃ³.

### 1.3 CÃ¡c bÆ°á»›c cÆ¡ báº£n thá»±c hiá»‡n má»™t thuáº­t toÃ¡n Há»c mÃ¡y:

NhÃ¬n chung, viá»‡c thá»±c hiá»‡n má»™t thuáº­t toÃ¡n Há»c mÃ¡y thÆ°á»ng tráº£i qua cÃ¡c bÆ°á»›c cÆ¡ báº£n sau:

1.	Thu tháº­p dá»¯ liá»‡u â€“ Gathering data/Data collection
2.	Tiá»n xá»­ lÃ½ dá»¯ liá»‡u â€“ Data preprocessing
    1.	TrÃ­ch xuáº¥t dá»¯ liá»‡u â€“ data extraction
    2.	LÃ m sáº¡ch dá»¯ liá»‡u â€“ data cleaning
    3.	Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u â€“ Data transformation
    4.	Chuáº©n hÃ³a dá»¯ liá»‡u â€“ Data normalization
    5.	TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng â€“ Feature extraction
3.	PhÃ¢n tÃ­ch dá»¯ liá»‡u â€“ Data analysis
4.	XÃ¢y dá»±ng mÃ´ hÃ¬nh mÃ¡y há»c â€“ Model building
5.	Huáº¥n luyá»‡n mÃ´ hÃ¬nh â€“ Model training
6.	ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh â€“ Model evaluation
   
Trong táº¥t cáº£ cÃ¡c bÆ°á»›c thÃ¬ viá»‡c thu tháº­p dá»¯ liá»‡u, tiá»n xá»­ lÃ½ vÃ  xÃ¢y dá»±ng bá»™ dá»¯ liá»‡u lÃ  tá»‘n nhiá»u thá»i gian vÃ  cÃ´ng sá»©c nháº¥t. ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng, cÃ³ áº£nh hÆ°á»Ÿng ráº¥t nhiá»u Ä‘áº¿n hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n Há»c mÃ¡y.

### 1.4 á»¨ng dá»¥ng cá»§a Há»c mÃ¡y:

á»¨ng dá»¥ng tá»•ng quÃ¡t:

â€¢	Xá»­ lÃ½ áº£nh
â€¢	PhÃ¢n tÃ­ch vÄƒn báº£n
â€¢	Khai phÃ¡ dá»¯ liá»‡u

á»¨ng dá»¥ng trong thá»±c táº¿:

â€¢	Giáº£i mÃ£ thá»‹ trÆ°á»ng tÃ i chÃ­nh

â€¢	Thay Ä‘á»•i cá»¥c diá»‡n ngÃ nh nÃ´ng nghiá»‡p

â€¢	NÃ¢ng cao hiá»‡u quáº£ vÃ  cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»‹ch vá»¥ ngÃ nh y táº¿

â€¢	CÆ¡ quan nhÃ  nÆ°á»›c cÃ³ thá»ƒ quáº£n lÃ½ tráº­t tá»± xÃ£ há»™i vÃ  Ä‘áº£m báº£o tÃ¬nh hÃ¬nh phÃ¡t triá»ƒn Ä‘áº¥t nÆ°á»›c

## CHÆ¯Æ NG 2 â€“ CÃC PHÆ¯Æ NG PHÃP OPTIMIZER
### 2.1 Tá»•ng quan vá» Optimizer
#### 2.1.1 Optimizer lÃ  gÃ¬?
Optimizer hay Thuáº­t toÃ¡n tá»‘i Æ°u lÃ  cÆ¡ sá»Ÿ Ä‘á»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh Neural Network vá»›i má»¥c Ä‘Ã­ch â€œhá»câ€ Ä‘Æ°á»£c cÃ¡c feature (hay pattern) cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o, Ä‘á»ƒ tá»« Ä‘Ã³ cÃ³ thá»ƒ tÃ¬m má»™t táº­p cÃ¡c trá»ng sá»‘ (weights â€“ w) vÃ  ngÆ°á»¡ng (bias â€“ b) phÃ¹ há»£p hÆ¡n Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh. 

VÃ  cÃ³ thá»ƒ nÃ³i cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u (Optimizition Algorithm) lÃ  má»™t trong nhá»¯ng â€œháº¡t nhÃ¢nâ€ máº¡nh máº½ cá»§a háº§u háº¿t thuáº­t toÃ¡n Machine Learning. ÄÃ¢y má»™t quy trÃ¬nh Ä‘Æ°á»£c thá»±c hiá»‡n láº·p Ä‘i láº·p láº¡i báº±ng cÃ¡ch so sÃ¡nh cÃ¡c giáº£i phÃ¡p khÃ¡c nhau cho Ä‘áº¿n khi tÃ¬m tháº¥y má»™t giáº£i phÃ¡p tá»‘i Æ°u hoáº·c thá»a Ä‘Ã¡ng.

Äá»‘i vá»›i ká»¹ thuáº­t há»c sÃ¢u nÃ³i riÃªng, thuáº­t toÃ¡n tá»‘i Æ°u lÃ  cÃ¡c ká»¹ thuáº­t giÃºp xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh máº¡ng nÆ¡-ron Ä‘á»ƒ tá»‘i Æ°u hÃ³a Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh máº¡ng.

<img src="picture/2.1.png">

HÃ¬nh 2.1 Minh há»a thuáº­t toÃ¡n tá»‘i Æ°u (Optimizer)

#### 2.1.2 Vai trÃ² cá»§a thuáº­t toÃ¡n tá»‘i Æ°u

Trong thuáº­t toÃ¡n há»c mÃ¡y nÃ³i chung vÃ  ká»¹ thuáº­t há»c sÃ¢u nÃ³i riÃªng, thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a (Optimizer) lÃ  má»™t khÃ¢u quan trá»ng khÃ´ng thá»ƒ thiáº¿u. QuÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a thá»±c hiá»‡n xÃ¡c Ä‘á»‹nh hÃ m máº¥t mÃ¡t (loss function) vÃ  sau Ä‘Ã³ tá»‘i thiá»ƒu hÃ³a hÃ m trÃªn báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m tá»‘i Æ°u. Cá»¥ thá»ƒ, thÃ´ng qua viá»‡c cáº­p nháº­t cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh (w, b) vÃ  Ä‘Ã¡nh giÃ¡ láº¡i hÃ m máº¥t mÃ¡t vá»›i má»™t tá»‰ lá»‡ há»c (learning rate) xÃ¡c Ä‘á»‹nh, quÃ¡ trÃ¬nh tá»‘i Æ°u giÃºp mÃ´ hÃ¬nh tÆ°Æ¡ng thÃ­ch tá»‘t hÆ¡n vá»›i táº­p dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘Ã o táº¡o.

##### 2.1.2.1 HÃ m máº¥t mÃ¡t (Loss function)

HÃ m máº¥t mÃ¡t (Loss function) lÃ  má»™t phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ Ä‘á»™ hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n â€œhá»câ€ cho mÃ´ hÃ¬nh trÃªn táº­p dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng. 
HÃ m máº¥t mÃ¡t tráº£ vá» má»™t sá»‘ thá»±c khÃ´ng Ã¢m thá»ƒ hiá»‡n sá»± chÃªnh lá»‡ch giá»¯a hai Ä‘áº¡i lÆ°á»£ng: 

â€¢	a: nhÃ£n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n

â€¢	y: nhÃ£n Ä‘Ãºng

Báº£n thÃ¢n hÃ m máº¥t mÃ¡t chÃ­nh lÃ  má»™t cÆ¡ cháº¿ thÆ°á»Ÿng-pháº¡t, mÃ´ hÃ¬nh sáº½ pháº£i Ä‘Ã³ng pháº¡t má»—i láº§n dá»± Ä‘oÃ¡n sai vÃ  má»©c pháº¡t tá»‰ lá»‡ thuáº­n vá»›i Ä‘á»™ lá»›n sai sÃ³t. 
Trong má»i bÃ i toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t, má»¥c tiÃªu luÃ´n bao gá»“m giáº£m tá»•ng má»©c pháº¡t pháº£i Ä‘Ã³ng. Trong trÆ°á»ng há»£p lÃ½ tÆ°á»Ÿng a = y, hÃ m máº¥t mÃ¡t sáº½ tráº£ vá» giÃ¡ trá»‹ cá»±c tiá»ƒu báº±ng 0. 
Hai hÃ m máº¥t mÃ¡t thÆ°á»ng xuyÃªn Ä‘Æ°á»£c sá»­ dá»¥ng trong máº¡ng nÆ¡-ron: 

â€¢	MSE (Mean Squared Error)

â€¢	Cross Entropy

##### 2.1.2.2 Tá»‰ lá»‡ há»c (Learning rate)

Learning rate hay tá»‰ lá»‡ há»c lÃ  má»™t thÃ´ng sá»‘ quan trá»ng trong viá»‡c quyáº¿t Ä‘á»‹nh tá»‘c Ä‘á»™ há»c cá»§a máº¡ng nÆ¡-ron. Tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c thá»ƒ hiá»‡n báº±ng sá»± thay Ä‘á»•i giÃ¡ trá»‹ cáº­p nháº­t trá»ng sá»‘ (weights - w) trong cÃ¡c chu ká»³ há»c. TÃ¹y theo má»¥c Ä‘Ã­ch cá»§a mÃ´ hÃ¬nh mÃ  tÄƒng/ giáº£m tá»‰ lá»‡ há»c. 

Tá»‰ lá»‡ há»c cÃ ng cao thÃ¬ giÃºp mÃ´ hÃ¬nh há»c khÃ¡ nhanh vÃ  tiáº¿t kiá»‡m Ä‘Æ°á»£c thá»i gian huáº¥n luyá»‡n, tuy nhiÃªn viá»‡c tá»‰ lá»‡ há»c lá»›n Ä‘á»“ng nghÄ©a vá»›i viá»‡c sá»± thay Ä‘á»•i trá»ng sá»‘ (weights - w) vÃ  tham sá»‘ ngÆ°á»¡ng (bias - b) cÃ ng lá»›n, mÃ´ hÃ¬nh khÃ´ng á»•n Ä‘á»‹nh, má»™t sá»‘ chu ká»³ há»c cÃ³ sá»± dao Ä‘á»™ng máº¡nh á»Ÿ tá»‰ lá»‡ nháº­n dáº¡ng Ä‘Ãºng hay nÃ³i cÃ¡ch khÃ¡c lÃ  thuáº­t toÃ¡n khÃ´ng Ä‘Æ°á»£c tá»‘i Æ°u vÃ  ngÆ°á»£c láº¡i Ä‘á»‘i vá»›i tá»‰ lá»‡ há»c nhá».

#### 2.1.3 Yáº¿u tá»‘ Ä‘Ã¡nh giÃ¡ má»™t thuáº­t toÃ¡n tá»‘i Æ°u

Má»™t vÃ i cÃ¡c yáº¿u tá»‘ hay Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»™t thuáº­t toÃ¡n tá»‘i Æ°u (Optimizer):

â€¢	Há»™i tá»¥ nhanh (trong quÃ¡ trÃ¬nh train)

â€¢	Sá»± tá»•ng quÃ¡t hÃ³a cao (váº«n nháº­n dáº¡ng Ä‘Æ°á»£c nhá»¯ng máº«u chÆ°a tá»«ng Ä‘Æ°á»£c huáº¥n luyá»‡n)

â€¢	Äá»™ chÃ­nh xÃ¡c cao

### 2.2 Má»™t sá»‘ thuáº­t toÃ¡n tá»‘i Æ°u (Optimization Algorithms)

Má»™t sá»‘ thuáº­t toÃ¡n tá»‘i Æ°u phá»• biáº¿n:

1.	Gradient Descent
2.	SGD vá»›i Ä‘á»™ng lÆ°á»£ng
3.	RMSProp
4.	Adagrad
5.	Adadelta
6.	Adam
7.	AdamW
8.	AMSGrad
   
#### 2.2.1 Gradient Descent (GD)

Gradient Descent (GD) lÃ  thuáº­t toÃ¡n tÃ¬m tá»‘i Æ°u chung cho cÃ¡c hÃ m sá»‘. Ã tÆ°á»Ÿng chung cá»§a GD lÃ  Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ Ä‘á»ƒ láº·p Ä‘i láº·p láº¡i thÃ´ng qua má»—i dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ giáº£m thiá»ƒu hÃ m chi phÃ­. 

ğ‘¤(ğ‘˜+1) = ğ‘¤(ğ‘˜) âˆ’ ğœ‚ âˆ‡ğ‘¤ ğ½(ğ‘¤(ğ‘˜) )

Trong Ä‘Ã³:

â€¢	ğ‘¤(ğ‘˜) : tham sá»‘ táº¡i bÆ°á»›c cáº­p nháº­t táº¡i lá»›p k

â€¢	Î—: tá»‰ lá»‡ há»c

â€¢	ğ½(ğ‘¤): hÃ m lá»—i

â€¢	âˆ‡ğ‘¤ ğ½(ğ‘¤ (ğ‘˜)): Ä‘áº¡o hÃ m cá»§a hÃ m lá»—i táº¡i Ä‘iá»ƒm ğ‘¤(ğ‘˜)

VÃ­ dá»¥:

```sh
from __future__ import division, print_function, unicode_literals
import math
import numpy as np 
import matplotlib.pyplot as plt

def grad(x):
    return 2*x+ 5*np.cos(x)

def cost(x):
    return x**2 + 5*np.sin(x)

def myGD1(eta, x0):
    x = [x0]
    for it in range(100):
        x_new = x[-1] - eta*grad(x[-1])
        if abs(grad(x_new)) < 1e-3:
            break
        x.append(x_new)
    return (x, it)

(x1, it1) = myGD1(.1, -5)
(x2, it2) = myGD1(.1, 5)
print('Solution x1 = %f, cost = %f, obtained after %d iterations'%(x1[-1], cost(x1[-1]), it1))
print('Solution x2 = %f, cost = %f, obtained after %d iterations'%(x2[-1], cost(x2[-1]), it2))
```
Káº¿t quáº£:

```sh
Solution x1 = -1.110667, cost = -3.246394, obtained after 11 iterations
Solution x2 = -1.110341, cost = -3.246394, obtained after 29 iterations
```
ï¶	Äiá»ƒm khá»Ÿi táº¡o khÃ¡c nhau

Sau khi cÃ³ cÃ¡c hÃ m cáº§n thiáº¿t, tÃ´i thá»­ tÃ¬m nghiá»‡m vá»›i cÃ¡c Ä‘iá»ƒm khá»Ÿi táº¡o khÃ¡c nhau lÃ  x0 = âˆ’5 vÃ  x0 = 5.

<img src="picture/2.2.png"> <img src="picture/2.2.1.png">

HÃ¬nh 2.2 Minh há»a thuáº­t toÃ¡n GD vá»›i Ä‘iá»ƒm khá»Ÿi táº¡o khÃ¡c nhau

Tá»« hÃ¬nh minh há»a trÃªn ta tháº¥y ráº±ng á»Ÿ hÃ¬nh bÃªn trÃ¡i, tÆ°Æ¡ng á»©ng vá»›i x0 =âˆ’5, nghiá»‡m há»™i tá»¥ nhanh hÆ¡n, vÃ¬ Ä‘iá»ƒm ban Ä‘áº§u x0 gáº§n vá»›i nghiá»‡m x* â‰ˆ âˆ’1 hÆ¡n. HÆ¡n ná»¯a, vá»›i x0 =5 á»Ÿ hÃ¬nh bÃªn pháº£i, Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m cÃ³ chá»©a má»™t khu vá»±c cÃ³ Ä‘áº¡o hÃ m khÃ¡ nhá» gáº§n Ä‘iá»ƒm cÃ³ hoÃ nh Ä‘á»™ báº±ng 2. 

=>	Äiá»u nÃ y khiáº¿n cho thuáº­t toÃ¡n la cÃ  á»Ÿ Ä‘Ã¢y khÃ¡ lÃ¢u. Khi vÆ°á»£t qua Ä‘Æ°á»£c Ä‘iá»ƒm nÃ y thÃ¬ má»i viá»‡c diá»…n ra ráº¥t tá»‘t Ä‘áº¹p.

ï¶	Learning rate khÃ¡c nhau

Tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a GD khÃ´ng nhá»¯ng phá»¥ thuá»™c vÃ o Ä‘iá»ƒm khá»Ÿi táº¡o ban Ä‘áº§u mÃ  cÃ²n phá»¥ thuá»™c vÃ o learning rate. 

VÃ­ dá»¥ vá»›i cÃ¹ng Ä‘iá»ƒm khá»Ÿi táº¡o x0 = âˆ’5 nhÆ°ng learning rate khÃ¡c nhau:

<img src="picture/2.3.png"> <img src="picture/2.3.1.png">

HÃ¬nh 2.3 Minh há»a thuáº­t toÃ¡n GD vá»›i Learning rate khÃ¡c nhau

Ta quan sÃ¡t tháº¥y hai Ä‘iá»u:

1.	Vá»›i learning rate nhá» Î·=0.01, tá»‘c Ä‘á»™ há»™i tá»¥ ráº¥t cháº­m. Trong vÃ­ dá»¥, do chá»n tá»‘i Ä‘a 100 vÃ²ng láº·p nÃªn thuáº­t toÃ¡n dá»«ng láº¡i trÆ°á»›c khi tá»›i Ä‘Ã­ch, máº·c dÃ¹ Ä‘Ã£ ráº¥t gáº§n. Trong thá»±c táº¿, khi viá»‡c tÃ­nh toÃ¡n trá»Ÿ nÃªn phá»©c táº¡p, learning rate quÃ¡ tháº¥p sáº½ áº£nh hÆ°á»Ÿng tá»›i tá»‘c Ä‘á»™ cá»§a thuáº­t toÃ¡n ráº¥t nhiá»u, tháº­m chÃ­ khÃ´ng bao giá» tá»›i Ä‘Æ°á»£c Ä‘Ã­ch.
2.	Vá»›i learning rate lá»›n Î·=0.5, thuáº­t toÃ¡n tiáº¿n ráº¥t nhanh tá»›i gáº§n Ä‘Ã­ch sau vÃ i vÃ²ng láº·p. Tuy nhiÃªn, thuáº­t toÃ¡n khÃ´ng há»™i tá»¥ Ä‘Æ°á»£c vÃ¬ bÆ°á»›c nháº£y quÃ¡ lá»›n, khiáº¿n nÃ³ cá»© quáº©n quanh á»Ÿ Ä‘Ã­ch.
   
ïƒ˜	Viá»‡c lá»±a chá»n learning rate ráº¥t quan trá»ng trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿. Viá»‡c lá»±a chá»n giÃ¡ trá»‹ nÃ y phá»¥ thuá»™c nhiá»u vÃ o tá»«ng bÃ i toÃ¡n vÃ  pháº£i lÃ m má»™t vÃ i thÃ­ nghiá»‡m Ä‘á»ƒ chá»n ra giÃ¡ trá»‹ tá»‘t nháº¥t. NgoÃ i ra, tÃ¹y vÃ o má»™t sá»‘ bÃ i toÃ¡n, GD cÃ³ thá»ƒ lÃ m viá»‡c hiá»‡u quáº£ hÆ¡n báº±ng cÃ¡ch chá»n ra learning rate phÃ¹ há»£p hoáº·c chá»n learning rate khÃ¡c nhau á»Ÿ má»—i vÃ²ng láº·p.

CÃ³ má»™t sá»‘ biáº¿n thá»ƒ khÃ¡c nhau cá»§a GD tÃ¹y thuá»™c vÃ o sá»‘ lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t. Gá»“m: 

1.	Batch Gradient Descent (Batch GD)
2.	Stochastic Gradient Descent (SGD)
3.	Mini-batch Gradient Descent (Mini-batch GD)
   
##### 2.2.1.1 Batch Gradient Descent (Batch GD)

Thuáº­t toÃ¡n Batch Gradient Descent (Batch GD) tÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t táº¡i w trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u. Táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh gradient trÆ°á»›c khi cáº­p nháº­t bá»™ trá»ng sá»‘ w. Háº¡n cháº¿ cá»§a Batch GD lÃ  khi táº­p dá»¯ liá»‡u lá»›n, viá»‡c tÃ­nh gradient sáº½ tá»‘n nhiá»u thá»i gian vÃ  chi phÃ­ tÃ­nh toÃ¡n.

##### 2.2.1.2 Stochastic Gradient Descent (SGD)

Äá»ƒ kháº¯c phá»¥c háº¡n cháº¿ cá»§a Bathc GD, thuáº­t toÃ¡n Stochastic Gradient Descent (SGD) thá»±c hiá»‡n viá»‡c cáº­p nháº­t trá»ng sá»‘ vá»›i má»—i máº«u dá»¯ liá»‡u x(i) cÃ³ nhÃ£n tÆ°Æ¡ng á»©ng y(i) nhÆ° sau: 

ğ‘¤(ğ‘˜+1) = ğ‘¤(ğ‘˜) âˆ’ ğœ‚ âˆ‡ğ‘¤ ğ½(ğ‘¤(ğ‘˜), x(i), y(i))

Vá»›i cÃ¡ch cáº­p nháº­t nÃ y, SGD thÆ°á»ng nhanh hÆ¡n Batch GD vÃ  cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ há»c trá»±c tuyáº¿n (online learning) khi táº­p dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c. 

Vá»›i SGD, bá»™ trá»ng sá»‘ w Ä‘Æ°á»£c cáº­p nháº­t thÆ°á»ng xuyÃªn hÆ¡n so vá»›i Batch GD vÃ  vÃ¬ váº­y hÃ m máº¥t mÃ¡t cÅ©ng dao Ä‘á»™ng nhiá»u hÆ¡n. Sá»± dao Ä‘á»™ng nÃ y khiáº¿n SGD cÃ³ váº» khÃ´ng á»•n Ä‘á»‹nh nhÆ°ng láº¡i cÃ³ Ä‘iá»ƒm tÃ­ch cá»±c lÃ  nÃ³ giÃºp di chuyá»ƒn Ä‘áº¿n nhá»¯ng Ä‘iá»ƒm cá»±c tiá»ƒu (Ä‘á»‹a phÆ°Æ¡ng) má»›i cÃ³ tiá»m nÄƒng hÆ¡n. Vá»›i tá»‘c Ä‘á»™ há»c giáº£m, kháº£ nÄƒng há»™i tá»¥ cá»§a SGD cÅ©ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Batch GD.

HÃ m sá»‘ trong Python Ä‘á»ƒ giáº£i Linear Regression theo SGD:

```sh
def sgrad(w, i, rd_id):
    true_i = rd_id[i]
    xi = Xbar[true_i, :]
    yi = y[true_i]
    a = np.dot(xi, w) - yi
    return (xi*a).reshape(2, 1)

def SGD(w_init, grad, eta):
    w = [w_init]
    w_last_check = w_init
    iter_check_w = 10
    N = X.shape[0]
    count = 0
    for it in range(10):  
# shuffle data 
        rd_id = np.random.permutation(N)
        for i in range(N):
            count += 1 
            g = sgrad(w[-1], i, rd_id)
            w_new = w[-1] - eta*g
            w.append(w_new)
            if count%iter_check_w == 0:
                w_this_check = w_new                 
                if np.linalg.norm(w_this_check - w_last_check)/len(w_init) < 1e-3:                                    
                    return w
                w_last_check = w_this_check
    return w
```

Káº¿t quáº£ thu Ä‘Æ°á»£c:

<img src="picture/2.4.png"> <img src="picture/2.4.1.png">
HÃ¬nh 2.4 TrÃ¡i: Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m vá»›i SGD. Pháº£i: giÃ¡ trá»‹ cá»§a Loss function táº¡i 50 vÃ²ng láº·p Ä‘áº§u tiÃªn.

##### 2.2.1.3 Mini-batch Gradient Descent (Mini-batch GD)

CÃ¡ch tiáº¿p cáº­n thá»© ba lÃ  thuáº­t toÃ¡n Mini-batch Gradient Descent (Mini-batch GD). KhÃ¡c vá»›i hai thuáº­t toÃ¡n trÆ°á»›c, Mini-batch GD sá»­ dá»¥ng t Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»ƒ cáº­p nháº­t bá»™ trá»ng sá»‘ (1<t<N) vá»›i N lÃ  tá»•ng sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u). 

ğ‘¤(ğ‘˜+1) = ğ‘¤(ğ‘˜) âˆ’ ğœ‚ âˆ‡ğ‘¤ ğ½(ğ‘¤(ğ‘˜), x(i: i+t), y(i: i+t))

Vá»›i x(i: i+t) Ä‘Æ°á»£c hiá»ƒu lÃ  dá»¯ liá»‡u tá»« thá»© i tá»›i thá»© i+tâˆ’1. Dá»¯ liá»‡u nÃ y sau má»—i epoch lÃ  khÃ¡c nhau vÃ¬ chÃºng cáº§n Ä‘Æ°á»£c xÃ¡o trá»™n. Má»™t láº§n ná»¯a, cÃ¡c thuáº­t toÃ¡n khÃ¡c cho GD nhÆ° Momentum, Adagrad, Adadelta, â€¦ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o Ä‘Ã¢y. GiÃ¡ trá»‹ t thÆ°á»ng Ä‘Æ°á»£c chá»n lÃ  khoáº£ng tá»« 50 Ä‘áº¿n 100. 

Mini-batch GD giáº£m sá»± dao Ä‘á»™ng cá»§a hÃ m máº¥t mÃ¡t so vá»›i SGD vÃ  chi phÃ­ tÃ­nh gradient vá»›i k Ä‘iá»ƒm dá»¯ liá»‡u lÃ  cháº¥p nháº­n Ä‘Æ°á»£c. 

Mini-batch GD thÆ°á»ng Ä‘Æ°á»£c lá»±a chá»n khi huáº¥n luyá»‡n máº¡ng nÆ¡ron vÃ  vÃ¬ váº­y trong má»™t sá»‘ trÆ°á»ng há»£p, SGD Ä‘Æ°á»£c hiá»ƒu lÃ  Mini-batch GD. RiÃªng báº£n thÃ¢n Mini-batch GD khÃ´ng Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m máº¥t mÃ¡t mÃ  bÃªn cáº¡nh Ä‘Ã³ cÃ¡c yáº¿u tá»‘ nhÆ° tá»‘c Ä‘á»™ há»c, thuá»™c tÃ­nh dá»¯ liá»‡u vÃ  tÃ­nh cháº¥t cá»§a hÃ m máº¥t mÃ¡t cÅ©ng áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘iá»u nÃ y.

VÃ­ dá»¥ vá» giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t má»—i khi cáº­p nháº­t tham sá»‘ w cá»§a má»™t bÃ i toÃ¡n khÃ¡c:

<img src="picture/2.5.png">
HÃ¬nh 2.5 VÃ­ dá»¥ vá» Mini-batch Gradient Descent

ïƒ˜	HÃ m máº¥t mÃ¡t nháº£y lÃªn nháº£y xuá»‘ng (fluctuate) sau má»—i láº§n cáº­p nháº­t nhÆ°ng nhÃ¬n chung giáº£m dáº§n vÃ  cÃ³ xu hÆ°á»›ng há»™i tá»¥ vá» cuá»‘i.

#### 2.2.2 SGD vá»›i Ä‘á»™ng lÆ°á»£ng (SGD with momentum)

SGD vá»›i momentum lÃ  phÆ°Æ¡ng phÃ¡p giÃºp tÄƒng tá»‘c cÃ¡c vectÆ¡ Ä‘á»™ dá»‘c theo Ä‘Ãºng hÆ°á»›ng, vÃ  giÃºp há»‡ thá»‘ng há»™i tá»¥ nhanh hÆ¡n. ÄÃ¢y lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a phá»• biáº¿n nháº¥t vÃ  nhiá»u mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i sá»­ dá»¥ng nÃ³ Ä‘á»ƒ Ä‘Ã o táº¡o. 

MÃ´ táº£ nhÆ° sau: 

ğ‘£ğ‘— â† ğ›¼ âˆ— ğ‘£ğ‘— âˆ’ ğœ‚ âˆ— ğ›»ğ‘Š âˆ‘_1^mâ–’ã€–L_m (w) ã€— 

ğ‘¤ğ‘— â† ğ‘£ğ‘— + ğ‘¤ğ‘—

PhÆ°Æ¡ng trÃ¬nh cÃ³ hai pháº§n. Trong Ä‘Ã³:

	vj: Ä‘á»™ dá»‘c Ä‘Æ°á»£c giá»¯ láº¡i tá»« cÃ¡c láº§n láº·p trÆ°á»›c
 
	Há»‡ sá»‘ Ä‘á»™ng lÆ°á»£ng Î±: tá»‰ lá»‡ pháº§n trÄƒm cá»§a Ä‘á»™ dá»‘c Ä‘Æ°á»£c giá»¯ láº¡i má»—i láº§n láº·p
 
	L: hÃ m máº¥t mÃ¡t
 
	Î·: tá»‰ lá»‡ há»c

#### 2.2.3 RMSProp (Root Mean Square Propogation)

RMSProp sá»­ dá»¥ng trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng cá»§a gradient Ä‘á»ƒ chuáº©n hÃ³a gradient. CÃ³ tÃ¡c dá»¥ng cÃ¢n báº±ng kÃ­ch thÆ°á»›c bÆ°á»›c - giáº£m bÆ°á»›c cho Ä‘á»™ dá»‘c lá»›n Ä‘á»ƒ trÃ¡nh hiá»‡n tÆ°á»£ng phÃ¡t ná»• Ä‘á»™ dá»‘c (Exploding Gradient), vÃ  tÄƒng bÆ°á»›c cho Ä‘á»™ dá»‘c nhá» Ä‘á»ƒ trÃ¡nh biáº¿n máº¥t Ä‘á»™ dá»‘c (Vanishing Gradient). RMSProp tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c táº­p, vÃ  chá»n má»™t tá»‰ lá»‡ há»c táº­p khÃ¡c nhau cho má»—i tham sá»‘. 

PhÆ°Æ¡ng phÃ¡p cáº­p nháº­t cÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° mÃ´ táº£:

ğ‘ ğ‘¡ = ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ ğœŒ) âˆ— g_t^2

ğ›¥ğ‘¥ğ‘¡ = -Î·/âˆš(s_t  + Ïµ) âˆ— ğ‘”ğ‘¡ 

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ + ğ›¥ğ‘¥t

Trong Ä‘Ã³:

	ğ‘ ğ‘¡: tÃ­ch luá»¹ phÆ°Æ¡ng sai cá»§a cÃ¡c gradient trong quÃ¡ khá»©
 
	ğœŒ: tham sá»‘ suy giáº£m
 
	ğ›¥ğ‘¥ğ‘¡: sá»± thay Ä‘á»•i cÃ¡c tham sá»‘ trong mÃ´ hÃ¬nh
 
	ğ‘”ğ‘¡: gradient cá»§a cÃ¡c tham sá»‘ táº¡i vÃ²ng láº·p t
 
	Ïµ: tham sá»‘ Ä‘áº£m báº£o káº¿t quáº£ xáº¥p xá»‰ cÃ³ Ã½ nghÄ©a.
 
#### 2.2.4 Adagrad

Adagrad lÃ  má»™t ká»¹ thuáº­t há»c mÃ¡y tiÃªn tiáº¿n, thá»±c hiá»‡n giáº£m dáº§n Ä‘á»™ dá»‘c báº±ng cÃ¡ch thay Ä‘á»•i tá»‘c Ä‘á»™ há»c táº­p. Adagrad Ä‘Æ°á»£c cáº£i thiá»‡n hÆ¡n báº±ng cÃ¡ch cho trá»ng sá»‘ há»c táº­p chÃ­nh xÃ¡c dá»±a vÃ o Ä‘áº§u vÃ o trÆ°á»›c nÃ³ Ä‘á»ƒ tá»± Ä‘iá»u chá»‰nh tá»‰ lá»‡ há»c theo hÆ°á»›ng tá»‘i Æ°u nháº¥t thay vÃ¬ vá»›i má»™t tá»‰ lá»‡ há»c duy nháº¥t cho táº¥t cáº£ cÃ¡c nÃºt.

Thuáº­t toÃ¡n Adagrad Ä‘Æ°á»£c Duchi J. vÃ  cÃ¡c cá»™ng sá»± Ä‘á» xuáº¥t nÄƒm 2011. KhÃ¡c vá»›i SGD, tá»‘c Ä‘á»™ há»c trong Adagrad thay Ä‘á»•i tÃ¹y thuá»™c vÃ o trá»ng sá»‘: tá»‘c Ä‘á»™ há»c tháº¥p Ä‘á»‘i vá»›i cÃ¡c trá»ng sá»‘ tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘áº·c trÆ°ng phá»• biáº¿n, tá»‘c Ä‘á»™ há»c cao Ä‘á»‘i vá»›i cÃ¡c trá»ng sá»‘ tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘áº·c trÆ°ng Ã­t phá»• biáº¿n.

gt, i  = âˆ‡wJ (wt, i)

Trong Ä‘Ã³:

	gt: gradient cá»§a hÃ m máº¥t mÃ¡t táº¡i bÆ°á»›c t
 
	gt, i :  Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m máº¥t mÃ¡t theo wi táº¡i bÆ°á»›c t
 
Quy táº¯c cáº­p nháº­t cá»§a Adagrad:

wğ‘¡+1, i = wt, i -Î·/âˆš(G_(t,ii)  + Ïµ) âˆ— ğ‘”ğ‘¡, i

Theo quy táº¯c cáº­p nháº­t, Adagrad Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c Î· táº¡i bÆ°á»›c t tÆ°Æ¡ng á»©ng vá»›i trá»ng sá»‘ wi xÃ¡c Ä‘á»‹nh dá»±a trÃªn cÃ¡c gradient Ä‘Ã£ tÃ­nh Ä‘Æ°á»£c theo wi. 

	Máº«u sá»‘ lÃ  chuáº©n L2 (L2 norm) cá»§a ma tráº­n Ä‘Æ°á»ng chÃ©o Gt trong Ä‘Ã³ pháº§n tá»­ i,i lÃ  tá»•ng bÃ¬nh phÆ°Æ¡ng cá»§a cÃ¡c gradient tÆ°Æ¡ng á»©ng vá»›i wi tÃ­nh Ä‘áº¿n bÆ°á»›c t.
 
	Îµ lÃ  má»™t sá»‘ dÆ°Æ¡ng khÃ¡ nhá» nháº±m trÃ¡nh trÆ°á»ng há»£p máº«u sá»‘ báº±ng 0.
 
Quy táº¯c cáº­p nháº­t trÃªn cÃ³ thá»ƒ viáº¿t dÆ°á»›i dáº¡ng tá»•ng quÃ¡t hÆ¡n nhÆ° sau:

wğ‘¡+1 = wt -Î·/âˆš(G_t  + Ïµ) â¨€ğ‘”ğ‘¡

Trong Ä‘Ã³, â¨€ lÃ  phÃ©p nhÃ¢n ma tráº­n-vectÆ¡ giá»¯a Gt vÃ  gt . 

CÃ³ thá»ƒ nháº­n tháº¥y ráº±ng trong thuáº­t toÃ¡n Adagrad tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh. Adagrad thÆ°á»ng khÃ¡ hiá»‡u quáº£ Ä‘á»‘i vá»›i bÃ i toÃ¡n cÃ³ dá»¯ liá»‡u phÃ¢n máº£nh. Tuy nhiÃªn, háº¡n cháº¿ cá»§a Adagrad lÃ  cÃ¡c tá»•ng bÃ¬nh phÆ°Æ¡ng á»Ÿ máº«u sá»‘ ngÃ y cÃ ng lá»›n khiáº¿n tá»‘c Ä‘á»™ há»c ngÃ y cÃ ng giáº£m vÃ  cÃ³ thá»ƒ tiá»‡m cáº­n Ä‘áº¿n giÃ¡ trá»‹ 0 khiáº¿n cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n gáº§n nhÆ° Ä‘Ã³ng bÄƒng. BÃªn cáº¡nh Ä‘Ã³, giÃ¡ trá»‹ tá»‘c Ä‘á»™ há»c Î· cÅ©ng pháº£i Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh má»™t cÃ¡ch thá»§ cÃ´ng.

#### 2.2.5 Adadelta

Thuáº­t toÃ¡n Adadelta Ä‘Æ°á»£c Zeiler vÃ  cÃ¡c cá»™ng sá»± Ä‘á» xuáº¥t nÄƒm 2012. Adadelta lÃ  má»™t biáº¿n thá»ƒ cá»§a Adagrad Ä‘á»ƒ kháº¯c phá»¥c tÃ¬nh tráº¡ng giáº£m tá»‘c Ä‘á»™ há»c á»Ÿ Adagrad. Adadelta khÃ´ng cÃ³ tham sá»‘ tá»‰ lá»‡ há»c cho nÃªn, thay vÃ¬ lÆ°u láº¡i táº¥t cáº£ gradient nhÆ° Adagrad, Adadelta giá»›i háº¡n tÃ­ch lÅ©y gradient theo cá»­a sá»• cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh cá»§a trá»ng sá»‘ w. Báº±ng cÃ¡ch nÃ y, Adadelta váº«n tiáº¿p tá»¥c há»c sau nhiá»u bÆ°á»›c cáº­p nháº­t.

g_t^'= âˆš((ã€–Î”xã€—_(t-1)+Ïµ)/(s_t+Ïµ)) *ğ‘”ğ‘¡ 

ğ‘¥ğ‘¡ = ğ‘¥ğ‘¡âˆ’1 âˆ’ g_t^' 

ğ›¥ğ‘¥ğ‘¡ = ğœŒğ›¥ğ‘¥ğ‘¡âˆ’1 + (1 âˆ’ ğœŒ) x_t^2

Tá»« cÃ´ng thá»©c, Adadelta sá»­ dá»¥ng 2 biáº¿n tráº¡ng thÃ¡i: 

	ğ‘ ğ‘¡: Ä‘á»ƒ lÆ°u trá»¯ trung bÃ¬nh cá»§a khoáº£ng thá»i gian thá»© hai cá»§a gradient vÃ  Î”ğ‘¥ğ‘¡ Ä‘á»ƒ lÆ°u trá»¯ trung bÃ¬nh cá»§a khoáº£ng thá»i gian thá»© 2 cá»§a sá»± thay Ä‘á»•i cÃ¡c tham sá»‘ trong mÃ´ hÃ¬nh. 
 
	g_t^': cÄƒn báº­c hai thÆ°Æ¡ng cá»§a trung bÃ¬nh tá»‘c Ä‘á»™ thay Ä‘á»•i bÃ¬nh phÆ°Æ¡ng vÃ  trung bÃ¬nh mÃ´-men báº­c hai cá»§a gradient. 
 
#### 2.2.6 Adam

Adam Ä‘Æ°á»£c xem nhÆ° lÃ  sá»± káº¿t há»£p cá»§a RMSprop vÃ  Stochastic Gradient Descent vá»›i Ä‘á»™ng lÆ°á»£ng. Adam lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‰ lá»‡ há»c thÃ­ch á»©ng, nÃ³ tÃ­nh toÃ¡n tá»‰ lá»‡ há»c táº­p cÃ¡ nhÃ¢n cho cÃ¡c tham sá»‘ khÃ¡c nhau. Adam sá»­ dá»¥ng Æ°á»›c tÃ­nh cá»§a khoáº£ng thá»i gian thá»© nháº¥t vÃ  thá»© hai cá»§a Ä‘á»™ dá»‘c Ä‘á»ƒ Ä‘iá»u chá»‰nh tá»‰ lá»‡ há»c cho tá»«ng trá»ng sá»‘ cá»§a máº¡ng nÆ¡-ron.

Tuy nhiÃªn, qua nghiÃªn cá»©u thá»±c nghiá»‡m, trong má»™t sá»‘ trÆ°á»ng há»£p, Adam váº«n cÃ²n gáº·p pháº£i nhiá»u thiáº¿u sÃ³t so vá»›i thuáº­t toÃ¡n SGD. Thuáº­t toÃ¡n Adam Ä‘Æ°á»£c mÃ´ táº£: 

ğ‘šğ‘¡ = ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ ğ›½1)ğ‘”ğ‘¡

ğ‘£ğ‘¡ = ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ ğ›½2) g_t^2

Trong Ä‘Ã³:

	vt lÃ  trung bÃ¬nh Ä‘á»™ng cá»§a bÃ¬nh phÆ°Æ¡ng
 
	mt lÃ  trung bÃ¬nh Ä‘á»™ng cá»§a gradient
 
	Î²1vÃ  Î²2 lÃ  tá»‘c Ä‘á»™ cá»§a di chuyá»ƒn
 
#### 2.2.7 AdamW

AdamW lÃ  má»™t biáº¿n thá»ƒ cá»§a Adam. Ã tÆ°á»Ÿng cá»§a AdamW khÃ¡ Ä‘Æ¡n giáº£n: khi thá»±c hiá»‡n thuáº­t toÃ¡n Adam vá»›i L2 regularization (chuáº©n hÃ³a L2), tÃ¡c giáº£ loáº¡i bá» pháº§n tiÃªu biáº¿n cá»§a trá»ng sá»‘ (weight decay) wtÎ¸t khá»i cÃ´ng thá»©c tÃ­nh gradient hÃ m máº¥t mÃ¡t táº¡i thá»i Ä‘iá»ƒm t: 

gt = ğ›¥f(Î¸t) + wtÎ¸t

vÃ  thay vÃ o Ä‘Ã³, Ä‘Æ°a pháº§n giÃ¡ trá»‹ Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n tÃ¡ch nÃ y vÃ o quÃ¡ trÃ¬nh cáº­p nháº­t trá»ng sá»‘: 

Î¸_(t+1,i)=Î¸_(t,i)-Î·(1/âˆš((v_t ) Ì‚+Ïµ)*(m_t ) Ì‚+w_(t,i) Î¸_(t,i) ),âˆ€t

#### 2.2.8 AMSGrad

AMSGrad sá»­ dá»¥ng giÃ¡ trá»‹ lá»›n nháº¥t cá»§a cÃ¡c bÃ¬nh phÆ°Æ¡ng gradient trÆ°á»›c Ä‘Ã³ vt Ä‘á»ƒ cáº­p nháº­t cÃ¡c trá»ng sá»‘. á» Ä‘Ã¢y, vt cÅ©ng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° trong thuáº­t toÃ¡n Adam: 

ğ‘£ğ‘¡ = ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ ğ›½2) g_t^2

Thay vÃ¬ trá»±c tiáº¿p sá»­ dá»¥ng vt (hay giÃ¡ trá»‹ Æ°á»›c lÆ°á»£ng (v_t ) Ì‚ ), thuáº­t toÃ¡n sáº½ sá»­ dá»¥ng giÃ¡ trá»‹ trÆ°á»›c Ä‘Ã³ vt-1 náº¿u giÃ¡ trá»‹ nÃ y lá»›n hÆ¡n giÃ¡ trá»‹ hiá»‡n táº¡i: 

(v_t ) Ì‚=maxâ¡((v_(t-1) ) Ì‚,v_t) 

TÆ°Æ¡ng tá»± nhÆ° trong thuáº­t toÃ¡n Adam, cÃ¡c giÃ¡ trá»‹ Ä‘Æ°á»£c Æ°á»›c lÆ°á»£ng theo cÃ´ng thá»©c dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ khá»­ lá»‡ch cho cÃ¡c trá»ng sá»‘:

ğ‘šğ‘¡ = ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ ğ›½1)ğ‘”ğ‘¡

ğ‘£ğ‘¡ = ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ ğ›½2) g_t^2

(v_t ) Ì‚=maxâ¡((v_(t-1) ) Ì‚,v_t) 

AMSGrad Ä‘Æ°á»£c cáº­p nháº­t theo quy táº¯c:

wğ‘¡+1 = wt -Î·/âˆš((v_t ) Ì‚  + Ïµ)*mğ‘¡






